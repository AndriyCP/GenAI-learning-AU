{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Transformers: Powerful NLP Tools for Data Analytics\n",
    "Learning about the HuggingFace Transformer models and Python package is essential for data analytics students, as it provides  access to state-of-the-art natural language processing (NLP) capabilities that can be easily integrated into their data analysis workflows.<P>\n",
    "The Transformers package (and corresponding model hub) offers pre-trained models for a variety of NLP tasks, enabling students to quickly incorporate powerful text processing functionalities into their projects, while also learning how to fine-tune and customize these models to specific domains and problems. Familiarity with Transformers equips data analytics students with knowledge of cutting-edge language modeling techniques, which are increasingly important as data sources become more text-based, preparing them for the evolving landscape of data-driven insights in the era of advanced deep learning models.<P>\n",
    "\n",
    "The Hugging Face Model Hub is a platform where the community can share and collaborate on trained models. The models themselves are typically open-source and free to use, but you should always check the specific license attached to each model for any restrictions or requirements. The license information is usually included in the model's page on the Model Hub.\n",
    "\n",
    "However, while the models are free to use, keep in mind that using the Hugging Face API to access these models may not be free. Hugging Face offers a certain amount of free API calls, but beyond that limit, you'll need to pay.\n",
    "\n",
    "Also, while the models are typically open-source, the datasets used to train these models may not be. Some models may have been trained on proprietary or confidential data. Always check the documentation associated with each model for details about the training data.\n",
    "\n",
    "This workbook was adapted from this course: https://huggingface.co/learn/nlp-course/chapter1/1<P>\n",
    "\n",
    "Transformer models are used to solve all kinds of NLP tasks. Here are some examples:\n",
    "- Text Classification (sentiment analysis)\n",
    "- Zero-Shot Classification\n",
    "- Text Generation\n",
    "- Question Answering\n",
    "- Summarization\n",
    "\n",
    "We are going to start with a high-level tool called a 'pipeline'. This is the easiest way to start to use transformers.\n",
    "\n",
    "### Table of Contents <a name=\"top\"></a>\n",
    "1. [Use your first transformer pipeline: sentiment analysis using classification](#sentiment-analysis)\n",
    "2. [Zero-shot Classification](#zero-shot)\n",
    "3. [Text Generation](#text-generation)\n",
    "4. [Question Answering](#question-answering)\n",
    "5. [Summarization](#summarization)\n",
    "6. [What's inside a pipeline?](#pipeline)\n",
    "7. [Your assignment](#assign)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.31.0\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /opt/conda/lib/python3.10/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: autogluon.multimodal\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# We already have the transformers package installed, let's check it.\n",
    "%pip show transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sentiment analysis pipeline <a name=\"sentiment-analysis\"></a>\n",
    "In Hugging Face's Transformers library, a pipeline is a high-level, easy-to-use abstraction for performing tasks with transformer models. It encapsulates the complex process of applying a transformer model into a simple function call.\n",
    "\n",
    "A pipeline ties together several steps to make model predictions on inputs:\n",
    " - preprocessing\n",
    " - passing inputs through a model\n",
    " - postprocessing\n",
    "\n",
    "For example, if you're using a pipeline for text classification, you would input raw text, and the pipeline would handle tokenization, input formatting, model inference, and output interpretation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 14:10:50.632146: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import the pipeline from the transformers package\n",
    "# You can ignore any warnings. We can discuss them or even supress them if needed.\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "# Let's create a pipeline for the NLP task called \"sentiment analysis\". We will use a trusted model:\n",
    "# Here is the model card: https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english\n",
    "#\n",
    "sa = pipeline(\"sentiment-analysis\",model = \"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a 2023 Q4 Earning statement from Google's Sundar Pichai -- Chief Executive Officer\n",
    "data = '''\n",
    "Hello, everyone. Our results reflect strong momentum and product innovation continuing into 2024. \n",
    "Today, I'm going to talk about four main topics. One are investments in the AI, including how it's \n",
    "helping Search; two, subscriptions, which reached $15 billion in annual revenue, up five times since 2019.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9994671940803528}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Send the data to the pipeline and see the analysis result\n",
    "sa(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a fictitious negative earnings statment\n",
    "data = '''\n",
    "Unfortunately, we experienced several significant supply chain constraints during the quarter,\n",
    "which limited our ability to meet customer demand for our flagship product. This, combined with\n",
    "increased competition in our core market, resulted in lower-than-expected revenue and earnings \n",
    "for the period. We are working diligently to resolve these supply issues and strengthen our competitive\n",
    "positioning, but expect these headwinds to persist through at least the first half of 2023.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9996461868286133}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Send the data to the pipeline and see the analysis result\n",
    "sa(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also send a list of sentences and the pipeline will classify each item in the list\n",
    "data =  [\n",
    "\"Unfortunately, we experienced several significant supply chain constraints during the quarter, which limited our ability to meet customer demand for our flagship product.\",\n",
    "\"This, combined with increased competition in our core market, resulted in lower-than-expected revenue and earnings for the period.\",\n",
    "\"We are working diligently to resolve these supply issues and strengthen our competitive positioning.\",\n",
    "\"We see a bright future in 2024!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfortunately, we experienced .... {'label': 'NEGATIVE', 'score': 0.9997628331184387}\n",
      "This, combined with increased .... {'label': 'NEGATIVE', 'score': 0.9997612833976746}\n",
      "We are working diligently to r.... {'label': 'POSITIVE', 'score': 0.9739233255386353}\n",
      "We see a bright future in 2024.... {'label': 'POSITIVE', 'score': 0.9997740387916565}\n"
     ]
    }
   ],
   "source": [
    "# Inference all items in the list\n",
    "response = sa(data)\n",
    "# Print the first 30 characters of the sentence and the inference result\n",
    "for i,r in enumerate(response):\n",
    "    print(data[i][0:30]+'....',r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your turn. Practice using the sentiment-analysis pipeline\n",
    "[Top of Page](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot-classification <a name=\"zero-shot\"></a>\n",
    "Zero-shot classification is a type of NLP task where the model is asked to classify input into categories it has not seen during training. It's called \"zero-shot\" because the model receives zero examples of some classes before being asked to classify them.\n",
    "\n",
    "In the context of Hugging Face's Transformers, the zero-shot-classification pipeline is a powerful tool that allows you to perform this task. It leverages models that have been trained on a large corpus of text, learning a rich representation of the semantics of the language. This allows them to generalize well to new, unseen classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline \n",
    "\n",
    "# Model card: https://huggingface.co/facebook/bart-large-mnli\n",
    "# Larger model\n",
    "zsc = pipeline(\"zero-shot-classification\", model =\"facebook/bart-large-mnli\") # 1.63GB\n",
    "#\n",
    "# Similar smaller model\n",
    "#zsc = pipeline(\"zero-shot-classification\", model =\"valhalla/distilbart-mnli-12-1\") # 890 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': '\\nThis new oven has exceeded all my expectations. \\nThe intuitive control panel makes it a breeze to use, and the convection \\nheating cooks my meals to perfection every time. \\nI would highly recommend this product to anyone looking to upgrade their home cooking setup.\\n',\n",
       " 'labels': ['Product Reviews',\n",
       "  'Personal Diary Entries',\n",
       "  'Cooking Recipes',\n",
       "  'News Articles',\n",
       "  'Poetry',\n",
       "  'Fictional Short Stories',\n",
       "  'Legal Contracts',\n",
       "  'Political Speeches',\n",
       "  'Scientific Research Papers',\n",
       "  'Software Documentation'],\n",
       " 'scores': [0.305452436208725,\n",
       "  0.1289210170507431,\n",
       "  0.10184430330991745,\n",
       "  0.09160089492797852,\n",
       "  0.09086976200342178,\n",
       "  0.08533546328544617,\n",
       "  0.06718843430280685,\n",
       "  0.04665406420826912,\n",
       "  0.04468149319291115,\n",
       "  0.03745215758681297]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statement = '''\n",
    "This new oven has exceeded all my expectations. \n",
    "The intuitive control panel makes it a breeze to use, and the convection \n",
    "heating cooks my meals to perfection every time. \n",
    "I would highly recommend this product to anyone looking to upgrade their home cooking setup.\n",
    "'''\n",
    "\n",
    "classes=[\n",
    "\"Product Reviews\",\n",
    "\"Political Speeches\",\n",
    "\"Personal Diary Entries\",\n",
    "\"Scientific Research Papers\",\n",
    "\"Fictional Short Stories\",\n",
    "\"News Articles\",\n",
    "\"Cooking Recipes\",\n",
    "\"Poetry\",\n",
    "\"Legal Contracts\",\n",
    "\"Software Documentation\"\n",
    "]\n",
    "response = zsc(statement,candidate_labels=classes)\n",
    "# Print the raw result\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Statement:\n",
      " \n",
      "This new oven has exceeded all my expectations. \n",
      "The intuitive control panel makes it a breeze to use, and the convection \n",
      "heating cooks my meals to perfection every time. \n",
      "I would highly recommend this product to anyone looking to upgrade their home cooking setup.\n",
      " \n",
      "\n",
      "The most likely class is: Product Reviews with a probablity of: 0.305452436208725 \n",
      "\n",
      "All classes with corresponding probabilities\n",
      "Label: Product Reviews, Score: 0.305452436208725\n",
      "Label: Personal Diary Entries, Score: 0.1289210170507431\n",
      "Label: Cooking Recipes, Score: 0.10184430330991745\n",
      "Label: News Articles, Score: 0.09160089492797852\n",
      "Label: Poetry, Score: 0.09086976200342178\n",
      "Label: Fictional Short Stories, Score: 0.08533546328544617\n",
      "Label: Legal Contracts, Score: 0.06718843430280685\n",
      "Label: Political Speeches, Score: 0.04665406420826912\n",
      "Label: Scientific Research Papers, Score: 0.04468149319291115\n",
      "Label: Software Documentation, Score: 0.03745215758681297\n"
     ]
    }
   ],
   "source": [
    "# Print the result formatted\n",
    "print(\"Input Statement:\\n\", response['sequence'], \"\\n\")\n",
    "\n",
    "print('The most likely class is:', response['labels'][0], 'with a probablity of:', response['scores'][0],'\\n')\n",
    "print('All classes with corresponding probabilities')\n",
    "for label, score in zip(response['labels'], response['scores']):\n",
    "    print(f\"Label: {label}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your turn. Practice using the zero-shot classification pipeline\n",
    "[Top of Page](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation <a name=\"text-generation\"></a>\n",
    "Text generation is a common task in NLP that involves generating human-readable text. It's often used to create responses for chatbots, generate narratives for games, write articles, and more.\n",
    "\n",
    "In the context of Hugging Face's Transformers, the text-generation pipeline uses a language model to generate text. Given some input text, the model generates text that continues from the input. The generated text is intended to be a plausible continuation of the input text, as if the same author were writing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 14:12:32.283127: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# let's use OpenAI's GPT2, now an open source model. This was released in 2019\n",
    "# Model card: https://huggingface.co/openai-community/gpt2\n",
    "tg = pipeline(\"text-generation\", model = \"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " In a world where AI is commonplace, this is a massive victory. If you work for a high-tech startup like ours, you're not expected to spend long on human resources.\n",
      "\n",
      "These days, a startup is usually a handful of people, each with three or four employees who make very few money on the $20 a year market. If things go south, you might even miss out on a significant investment.\n",
      "\n",
      "\"Companies don't have to rely on robots for hiring and promotion,\" says Peter Klemman, president\n"
     ]
    }
   ],
   "source": [
    "# Give the pipeline a prompt and it will continue the text\n",
    "response = tg(\"In a world where AI is commonplace,\",max_new_tokens = 100)\n",
    "print('\\n',response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# The pipeline also has some parameters we can use to really customize the output.\n",
    "# They are documented here: https://huggingface.co/docs/transformers/en/main_classes/text_generation\n",
    "\n",
    "# My selection of parameters makes the output kind of wacky\n",
    "response = tg(\"In a world where AI is commonplace,\",\n",
    "              max_new_tokens = 100,\n",
    "              temperature=0.99, \n",
    "              top_k=50,\n",
    "              repetition_penalty=1.2,\n",
    "              num_return_sequences=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: 1 \n",
      "\n",
      " In a world where AI is commonplace, you might wonder why we even bother to look around.\n",
      "'Technology isn't the only challenge.' So if I was able (to take my own personal computing) in such an emergency scenario — one of us could be seriously injured by something big happening next year or possibly die from drowning this summer … that's not going away anytime soon,' Mr Kessel says about technology change and how it'll impact our behaviour… \n",
      "\n",
      "Response: 2 \n",
      "\n",
      " In a world where AI is commonplace, this would not be surprising. Humans are already trying to make progress as human innovation reaches its apogee of self-driving cars that allow drivers and passengers alike to travel across high speed highways in the blink—and with them one step closer toward solving many key problems we face today:\n",
      " \n",
      "\n",
      "Response: 3 \n",
      "\n",
      " In a world where AI is commonplace, it's not surprising to see that this will be the main mode of computing.\n",
      " Why? Because when humans make decisions they're guided and told by social cues (social media). When I spoke with Jens Schlosser about his company Visionary Computing Research last week he expressed some intriguing thoughts on how society shapes one generation vs another; which are different people versus each other or should our societies evolve from simple hierarchies like human beings do in many cases. We also saw for ourselves what happened \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This will have 3 generated responses\n",
    "for i,r in enumerate(response):\n",
    "    print('Response:', i+1,'\\n\\n',r['generated_text'],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To summarize the Star Wars Episode 3 movie is set in a galaxy far, distant future where all life on this planet has been wiped out. A group of smugglers who are seeking to escape from their ship have arrived and want us here too: they call themselves \"The Galactic Empire,\" but it's only when you begin working aboard your starship that we learn more about them... In fact I can't even fathom how other people like me could make sense of what these two guys mean by saying such things as \"...it looks just fine\" or something else entirely? And if everyone knew there was nothing wrong with being human before any one person ever did anything stupid (like throwing away his lightsaber), for example would those very same pirates then say no thanks?!\n",
      "\n",
      "\n",
      "Also - because The Force Awakens isn;t really an action-packed film -- not at least yet! So why do some fanboys keep going back into prequels without knowing everything so thoroughly??? Why does anyone still try again after reading up upon every detail which makes fans happy!?\n"
     ]
    }
   ],
   "source": [
    "# Just another example\n",
    "data = '''To summarize the Star Wars Episode 3 movie is'''\n",
    "response = tg(data,max_new_tokens = 200, temperature=.7, repetition_penalty=2.0)\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your turn. Practice using text generation.\n",
    "[Top of Page](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering <a name=\"question-answering\"></a>\n",
    "In NLP, question answering  is a task where the system is given a passage of text (the \"context\") and a question related to that text, and it tries to answer the question based on the information in the text.\n",
    "\n",
    "In the context of Hugging Face's Transformers, the question-answering pipeline is used for this task. It uses models that have been trained similar question-ansswering patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad\n",
    "qa = pipeline(\"question-answering\",model=\"distilbert/distilbert-base-cased-distilled-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.7949253916740417, 'start': 30, 'end': 38, 'answer': 'Cal Poly'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are many parameters you can customize the inference call, but at the minimum you need a context and question\n",
    "qa(context=\"My name is Kurt and I work at Cal Poly in San Luis Obispo.\",\n",
    "    question=\"Where does Kurt work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.5232281684875488, 'start': 30, 'end': 38, 'answer': 'Cal Poly'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# But, it has limitations\n",
    "qa(context=\"My name is Kurt and I work at Cal Poly in San Luis Obispo.\",\n",
    "    question=\"What is Kurt's favorite actor?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What other sports does pickleball incorporate elements from? Answer: tennis, badminton, and table tennis Score: 0.9531278610229492 \n",
      "\n",
      "Where was pickleball first invented? Answer: Bainbridge Island, Washington Score: 0.6088538765907288 \n",
      "\n",
      "Who are credited with inventing the sport of pickleball? Answer: Joel Pritchard, Bill Bell, and Barney McCallum Score: 0.986624002456665 \n",
      "\n",
      "Why did pickleball initially become popular, especially among retirees? Answer: relatively simple rules and the ability to play competitively at different skill levels Score: 0.2571156620979309 \n",
      "\n",
      "What is one reason for the rapid growth of pickleball in recent years? Answer: relatively simple rules and the ability to play competitively at different skill levels Score: 0.2947987914085388 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lots of options here: https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.QuestionAnsweringPipeline\n",
    "# Another slightly more complex example\n",
    "cx = '''Pickleball is a paddleball sport that combines elements of tennis, badminton, and table tennis. \n",
    "The game is played on a badminton-sized court with a perforated plastic ball and solid paddles. Pickleball was invented\n",
    "in 1965 on Bainbridge Island, Washington by Joel Pritchard, Bill Bell, and Barney McCallum as a way to provide an \n",
    "entertaining game for the whole family. The sport quickly gained popularity, especially among retirees, due to its \n",
    "relatively simple rules and the ability to play competitively at different skill levels. Today, pickleball is one of \n",
    "the fastest growing sports in North America, with millions of players of all ages participating across thousands of \n",
    "indoor and outdoor courts.'''\n",
    "\n",
    "questions = [\n",
    "\"What other sports does pickleball incorporate elements from?\",\n",
    "\"Where was pickleball first invented?\",\n",
    "\"Who are credited with inventing the sport of pickleball?\",\n",
    "\"Why did pickleball initially become popular, especially among retirees?\",\n",
    "\"What is one reason for the rapid growth of pickleball in recent years?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    response = qa(context = cx, question = q)\n",
    "    print(q, 'Answer:', response['answer'], 'Score:', response['score'], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your turn. Practice question answering here.\n",
    "[Top of Page](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization <a name=\"summarization\"></a>\n",
    "Summarization is a task NLP where the goal is to create a concise and meaningful summary of a longer text. The summary should retain the key points of the original text while significantly reducing its length.\n",
    "\n",
    "In the context of Hugging Face's Transformers, the summarization pipeline is used for this task. It uses a model that has been trained on a summarization task to generate summaries of input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "# Model card: https://huggingface.co/facebook/bart-large-cnn\n",
    "# Parameters: https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.SummarizationPipeline\n",
    "summer = pipeline(\"summarization\", model = \"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "America has changed dramatically during recent years. The number of graduates in traditional engineering disciplines has declined. There are declining offerings in engineering subjects dealing with infrastructure, the environment, and related issues. Rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering.\n"
     ]
    }
   ],
   "source": [
    "data = '''\n",
    "    America has changed dramatically during recent years. Not only has the number of \n",
    "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
    "    the premier American universities engineering curricula now concentrate on \n",
    "    and encourage largely the study of engineering science. As a result, there \n",
    "    are declining offerings in engineering subjects dealing with infrastructure, \n",
    "    the environment, and related issues, and greater concentration on high \n",
    "    technology subjects, largely supporting increasingly complex scientific \n",
    "    developments. While the latter is important, it should not be at the expense \n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other \n",
    "    industrial countries in Europe and Asia, continue to encourage and advance \n",
    "    the teaching of engineering. Both China and India, respectively, graduate \n",
    "    six and eight times as many traditional engineers as does the United States. \n",
    "    Other industrial countries at minimum maintain their output, while America \n",
    "    suffers an increasingly serious decline in the number of engineering graduates \n",
    "    and a lack of well-educated engineers.\n",
    "'''\n",
    "response = summer(data)\n",
    "print(response[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's inside a pipeline? <a name=\"pipeline\"></a>\n",
    "In summary, a Hugging Face pipeline is a high-level, easy-to-use abstraction for performing tasks with Transformer models. It encapsulates several steps into a single callable object, including tokenization, running the model, and post-processing the model's outputs.\n",
    "\n",
    "Here's a simplified view of what's inside a pipeline:\n",
    "\n",
    "**Tokenizer**: This is responsible for converting the input text into a format that the model can understand. This includes splitting the text into tokens, mapping tokens to their IDs, and creating the necessary input tensors.\n",
    "\n",
    "**Model**: This is the Transformer model that performs the actual task (e.g., text classification, named entity recognition, question answering, etc.). The model takes the tensors produced by the tokenizer and returns output tensors.\n",
    "\n",
    "**Post-processor**: This takes the output tensors from the model and converts them into a more user-friendly format. The exact nature of the post-processing depends on the specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's inside a pipeline?\n",
    "# summer.tokenizer\n",
    "# summer.model\n",
    "# summer.model.config\n",
    "# summer.device\n",
    "# summer.framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your assignment <a name=\"assign\"></a>\n",
    "\n",
    "Pick out your favorite NLP Pipleine from above and experiment with it here. Perhaps use a differnt model or give it different input. \n",
    "\n",
    "OR\n",
    "\n",
    "Find a Pipeline we have not implemented yet and see if you can read the docs and get it to work:\n",
    "\n",
    "https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#natural-language-processing\n",
    "\n",
    "[Top of Page](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face models take up local disk space.\n",
    "# This will delete all the cached models and free up disk space. \n",
    "# We are not done with Hugging Face models, so don't do it yet.\n",
    "def purgeHF():\n",
    "    # purge cache\n",
    "    !rm -rf ~/.cache/huggingface\n",
    "#purgeHF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Transformers, what can they do?",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
