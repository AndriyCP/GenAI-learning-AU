{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07b5f360-fae5-466c-8ba0-fceeffe3f89e",
   "metadata": {},
   "source": [
    "## Pipelines: Underneath the Hood\n",
    "A Hugging Face pipeline encapsulates several steps into a single callable object, including tokenization, running the model, and post-processing the model's outputs. Here's how you can perform each step individually:\n",
    "\n",
    "Tokenization: This is responsible for converting the input text into a format that the model can understand.\n",
    "\n",
    "Model: This is the Transformer model that performs the actual task. The model takes the tensors produced by the tokenizer and returns output tensors.\n",
    "\n",
    "Post-processor: This takes the output tensors from the model and converts them into a more user-friendly format.\n",
    "\n",
    "### Table of Contents <a name=\"top\"></a>\n",
    "1. [Pipeline Review](#review)\n",
    "2. [Tokenizer](#tokenizer)\n",
    "3. [Create a model and inference the tokens](#model)\n",
    "4. [Model Output](#output)\n",
    "5. [Your Assignment](#assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1826ecd4-5967-4630-9a6e-083186e3cb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 17:09:14.485929: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import everything we need up front\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0220e9a6-2ea9-4dd6-81e9-cf5ab1f1d629",
   "metadata": {},
   "source": [
    "## Review <a name=\"review\"></a>\n",
    "Here is a pipeline just like we did in the last notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6af51771-2504-486a-a2a4-1972fa207551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Recall\n",
    "sa = pipeline(\"sentiment-analysis\",model = \"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a1cbc5b-1db1-4ae6-838b-d609dfe18034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a list of a couple sentences to be sentiment analyzed\n",
    "raw_inputs = [\n",
    "    \"When my friends ask me about my experience in this class, my response is I really love it!\",\n",
    "    \"When I am honest, the biggest thing I dislike about this class is the long, boring lectures.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ba1192d-3375-470c-82b8-bb8033488e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998725652694702},\n",
       " {'label': 'NEGATIVE', 'score': 0.9992125034332275}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just like last notebook, let's analyze them with the pipeline\n",
    "sa(raw_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fa06f58-0e34-40fe-9664-92a45176194f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: DistilBertTokenizerFast(name_or_path='distilbert-base-uncased-finetuned-sst-2-english', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)\n",
      "\n",
      "\n",
      "Model Config: DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look inside the Pipeline\n",
    "print('Tokenizer:', sa.tokenizer)\n",
    "print('\\n\\nModel Config:', sa.model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39affb69-bf43-4f34-b2da-12588a43935a",
   "metadata": {},
   "source": [
    "### What is going on inside?\n",
    "![alt text](images/tokenizer.jpg)\n",
    "[Top of Page](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb9b257-bdf1-4293-b6c3-11094ba89b50",
   "metadata": {},
   "source": [
    "## Tokenizer <a name=\"tokenizer\"></a>\n",
    "A tokenizer in the context of AI is a crucial component that converts raw text data into a numeric format that can be processed by machine learning models. \n",
    "\n",
    "It achieves this by breaking down the input text into smaller, meaningful units called tokens, which can be individual words, sub-words, or characters. The tokenizer also maps each token to a unique identifier, typically an integer value.  Additionally, the tokens map to the vocab words of the trained model.\n",
    "\n",
    "Hugging Face provides a wide range of pre-trained tokenizers tailored to specific language models, which ensures that your text data is properly formatted and aligned with the expectations of the corresponding model, enabling you to leverage the power of state-of-the-art language models for your NLP tasks.\n",
    "\n",
    "In the context of Hugging Face transformer models, some of the common tokenizers used include:\n",
    "\n",
    "1. **BertTokenizer**: This tokenizer was developed for the BERT (Bidirectional Encoder Representations from Transformers) model. It tokenizes the input text into wordpieces, which are sub-word units that help handle out-of-vocabulary words.\n",
    "\n",
    "2. **GPT2Tokenizer**: This tokenizer was developed for the GPT-2 (Generative Pre-trained Transformer 2) model. It uses a byte-level Byte-Pair Encoding (BPE) algorithm to tokenize the input text.\n",
    "\n",
    "3. **RobertaTokenizer**: This tokenizer was developed for the RoBERTa (Robustly Optimized BERT Pretraining Approach) model. It is based on the BPE algorithm and is similar to the GPT2Tokenizer.\n",
    "\n",
    "4. **DistilBertTokenizer**: This tokenizer was developed for the DistilBERT model, which is a smaller and faster version of the BERT model. It uses the same tokenization approach as the BertTokenizer.\n",
    "\n",
    "These are just a few examples of the common tokenizers used in Hugging Face transformer models. The choice of tokenizer often depends on the specific model and the task at hand, as different tokenizers may have different strengths and weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee73100a-95a9-472d-9921-32f6e000ff4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create just a tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Use the same model as we did in the pipeline above\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "# But now, create  just the tokenizer\n",
    "tok = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b6de5ce-fd99-4699-acc9-a3c4ba3dbb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original raw_input:\n",
      "When my friends ask me about my experience in this class, my response is I really love it!\n",
      "When I am honest, the biggest thing I dislike about this class is the long, boring lectures.\n",
      "\n",
      "Here are the tokens:\n",
      "['when', 'my', 'friends', 'ask', 'me', 'about', 'my', 'experience', 'in', 'this', 'class', ',', 'my', 'response', 'is', 'i', 'really', 'love', 'it', '!']\n",
      "['when', 'i', 'am', 'honest', ',', 'the', 'biggest', 'thing', 'i', 'dislike', 'about', 'this', 'class', 'is', 'the', 'long', ',', 'boring', 'lectures', '.']\n"
     ]
    }
   ],
   "source": [
    "# Here is our original raw_inputs from above, just a list of sentences\n",
    "print('Original raw_input:')\n",
    "for s in raw_inputs:\n",
    "    print(s)\n",
    "# And here is how the tokenizer breaks it into text tokens. These are still words.\n",
    "print('\\nHere are the tokens:')\n",
    "for s in raw_inputs:\n",
    "    print(tok.tokenize(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e369d1-9c26-4947-90f3-1119715263b5",
   "metadata": {},
   "source": [
    "#### Convert words to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d611bd5-6482-44de-9bad-f4dfaddc0139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentences:\n",
      "Sentence 1 tensor([ 101, 2043, 2026, 2814, 3198, 2033, 2055, 2026, 3325, 1999, 2023, 2465,\n",
      "        1010, 2026, 3433, 2003, 1045, 2428, 2293, 2009,  999,  102]) \n",
      " size of this tensor: torch.Size([22])\n",
      "Sentence 2 tensor([  101,  2043,  1045,  2572,  7481,  1010,  1996,  5221,  2518,  1045,\n",
      "        18959,  2055,  2023,  2465,  2003,  1996,  2146,  1010, 11771,  8921,\n",
      "         1012,   102]) \n",
      " size of this tensor: torch.Size([22])\n",
      "Attention tensor: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# Convert the words to a numeric representation. This is the primary function of the tokenizer\n",
    "tok_inputs = tok(raw_inputs, return_tensors=\"pt\") # use pytorch tensors vs. tensor flow tensors\n",
    "#\n",
    "# Look at the sentences that are now tokenized as numbers\n",
    "print(\"Tokenized sentences:\")\n",
    "for i,t in  enumerate(tok_inputs['input_ids']):\n",
    "    print('Sentence',i+1, t, '\\n size of this tensor:', t.shape)\n",
    "#\n",
    "# Look at the attention mask, this is part of the input to the model tells the network which tokens to pay more attention to\n",
    "# Let's not talk about this here.\n",
    "print('Attention tensor:', tok_inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79daab7e-b369-47cd-9967-ca48ea729890",
   "metadata": {},
   "source": [
    "[Top of Page](#top)\n",
    "## Create a model and inference the tokens <a name=\"model\"> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb469853-6eec-431c-9ea2-285207d501ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Create the same model as used for both the pipeline and the tokenizer above\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint,output_hidden_states=True,output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df2349c6-0af3-4458-aadb-3f968605e9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the type of the outputs object? <class 'transformers.modeling_outputs.SequenceClassifierOutput'>\n",
      "What is the size of the output object? 3\n"
     ]
    }
   ],
   "source": [
    "# Inference the model. In other words, send it the tokenized sentences\n",
    "outputs = model(**tok_inputs) # Give the model all of our inputs (** just means unpack the dictionary)\n",
    "\n",
    "# The output from the model is a little complex, let's look at a few details\n",
    "print(\"What is the type of the outputs object?\", type(outputs))\n",
    "print(\"What is the size of the output object?\", len(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93566e6-1ace-4ab9-897c-21ba552f5d04",
   "metadata": {},
   "source": [
    "When you call this model, it returns a tuple with the following three elements:\n",
    "\n",
    "**Logits** (outputs.logits or outputs[0]): This is a tensor of shape (batch_size, num_labels). Since this is a binary classification model, num_labels is 2. The logits are the raw, unnormalized scores that the model assigns to each class. You can convert these into probabilities by applying the softmax function.\n",
    "\n",
    "**Hidden states** (outputs.hidden_states or outputs[1]): This is a tuple of tensors representing the hidden states from all layers of the model. This is only returned if you set output_hidden_states=True when creating the model. Each tensor in the tuple has shape (batch_size, sequence_length, hidden_size), and outputs.hidden_states[0] gives the initial embeddings.\n",
    "\n",
    "**Attention weights** (outputs.attentions or outputs[2]): This is a tuple of tensors representing the attention weights from all layers of the model. This is only returned if you set output_attentions=True when calling the model. Each tensor in the tuple has shape (batch_size, num_heads, sequence_length, sequence_length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd241482-8658-425d-818e-2f6a6c07316d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the hidden_state object: 7\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the hidden_states, the 2nd item in the tuple\n",
    "print('Size of the hidden_state object:', len(outputs.hidden_states))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cb6fe8-3cd3-4713-8433-b7d998e2290c",
   "metadata": {},
   "source": [
    "For this model, there are 7 hidden states.\n",
    "\n",
    "- outputs.hidden_states[0]: The initial embeddnig of the input tokens\n",
    "- outputs.hidden_states[1-6]: The 6 hidden layers in the model's neural network\n",
    "\n",
    "\n",
    "Each row in this tensor corresponds to a token in your input, and the values in the row are the elements of the vector that represents that token. The dimensionality of these vectors (the number of elements in each row) is the size of the hidden states in the model, which is typically 768 for base models like BERT-base or DistilBERT-base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba5ef185-4d28-429f-8ea1-382da9001536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 22, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the first hidden state; the initial sentence embedding\n",
    "outputs.hidden_states[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a40e9bd-91fb-4c48-90c9-3151515f11e9",
   "metadata": {},
   "source": [
    "Here's what each dimension represents:\n",
    "\n",
    "**First dimension** (2): This is typically the batch size, i.e., the number of samples that are processed together. In our case, we sent 2 sentences.\n",
    "\n",
    "**Second dimension** (22): This is usually the sequence length, i.e., the number of tokens in each sample. Here, each sentence contains 22 tokens.\n",
    "\n",
    "**Third dimension** (768): This is the size of the hidden states, i.e., the dimensionality of the vectors that represent each sentence. In models like BERT-base and DistilBERT-base, each token is represented by a vector of size 768.\n",
    "\n",
    "So, a tensor of shape [2, 22, 768] would represent a batch of 2 sentences, where each sample is a sequence of 22 tokens, and each token is represented by a vector of size 768. This is a common shape for the output of a transformer model like BERT or DistilBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cadba349-b320-4bc2-93ed-b22b50548c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: When my friends ask me about my experience in this class, my response is I really love it!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3549, -0.1386, -0.2253,  ...,  0.1536,  0.0748,  0.1310],\n",
       "         [-1.2662,  0.7665, -0.4574,  ..., -0.4349,  1.0169,  0.4746],\n",
       "         [ 0.4449,  0.5868, -1.0603,  ..., -1.1192, -0.1726, -0.1478],\n",
       "         ...,\n",
       "         [-0.5035, -0.6060, -0.0490,  ...,  0.6768,  0.2647,  0.0453],\n",
       "         [ 1.3130, -0.0888, -1.0304,  ...,  0.1210,  0.3533,  0.3376],\n",
       "         [-0.2599,  0.0690, -0.3147,  ..., -0.3003,  0.2748, -0.1766]],\n",
       "\n",
       "        [[ 0.3549, -0.1386, -0.2253,  ...,  0.1536,  0.0748,  0.1310],\n",
       "         [-1.2662,  0.7665, -0.4574,  ..., -0.4349,  1.0169,  0.4746],\n",
       "         [-0.2136,  0.3967, -0.3817,  ...,  0.4060,  0.8568,  0.3673],\n",
       "         ...,\n",
       "         [-0.1879,  0.2698, -0.6148,  ...,  0.2745, -0.0660, -0.0161],\n",
       "         [ 0.0614,  0.1503, -0.3501,  ...,  0.4288,  0.3237,  0.1198],\n",
       "         [-0.2599,  0.0690, -0.3147,  ..., -0.3003,  0.2748, -0.1766]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So, just look at the embedding of the first sentence\n",
    "print('Input sentence:', raw_inputs[0])\n",
    "# This is the 'embedding' of the first sentence. It will flow to the next hidden layer in the trained model.\n",
    "outputs.hidden_states[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea73100-bf12-49de-b002-a8f7c5821195",
   "metadata": {},
   "source": [
    "![alt text](images/heads.jpg)\n",
    "[Top of Page](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b8072-e0b1-4857-9e78-5bf26793335d",
   "metadata": {},
   "source": [
    "## Model output <a name=\"output\"> </a>\n",
    "\n",
    "From above remember the logits are the most valuable outputs when it comes to predicting the class of the sentence (positive or negative)\n",
    "\n",
    "**Logits** (outputs.logits or outputs[0]): This is a tensor of shape (batch_size, num_labels). Since this is a binary classification model, num_labels is 2. The logits are the raw, unnormalized scores that the model assigns to each class. You can convert these into probabilities by applying the softmax function.\n",
    "\n",
    "The **softmax** function is commonly used in machine learning models to convert a vector of real numbers into a probability distribution. Given an input vector x, the softmax function computes the exponential (exp) of every element in the vector, and then normalizes the result by dividing each exp(x_i) by the sum of the exponentials of all elements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fc79096-0d4d-47a6-99ae-99ed90dd4431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.3012,  4.6666],\n",
       "        [ 3.8892, -3.2567]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is our output or prediction of the Positive/Negative, but it is still un-normalized\n",
    "outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80c194ca-ef68-417d-839c-15c1b6347231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this model: {0: 'NEGATIVE', 1: 'POSITIVE'}\n",
      "Raw Probability Predictions in the tensors:\n",
      " tensor([[1.2743e-04, 9.9987e-01],\n",
      "        [9.9921e-01, 7.8742e-04]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "\n",
      " ['When my friends ask me about my experience in this class, my response is I really love it!', 'When I am honest, the biggest thing I dislike about this class is the long, boring lectures.'] \n",
      "\n",
      "Normalized Classification Result:\n",
      "When my friends ask me about my experience in this class, my response is I really love it!\n",
      "\tNegative Probability: 0.00012743103434331715 Positivie Probability: 0.9998725652694702\n",
      "When I am honest, the biggest thing I dislike about this class is the long, boring lectures.\n",
      "\tNegative Probability: 0.9992125034332275 Positivie Probability: 0.0007874164148233831\n",
      "\n",
      "Pipeline result from above:\n",
      " {'label': 'POSITIVE', 'score': 0.9998725652694702}\n",
      "\n",
      "Pipeline result from above:\n",
      " {'label': 'NEGATIVE', 'score': 0.9992125034332275}\n"
     ]
    }
   ],
   "source": [
    "# Let's postprocess the tokens (Convert logits to probabilities that we can understand)\n",
    "# Use the softmax function to normalize out logits\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "# Print results\n",
    "print('In this model:', model.config.id2label)\n",
    "#\n",
    "# Print raw data\n",
    "print('Raw Probability Predictions in the tensors:\\n', predictions, '\\n')\n",
    "print('\\n', raw_inputs,'\\n')\n",
    "#\n",
    "# Format the output so it is easy to read\n",
    "print(\"Normalized Classification Result:\")\n",
    "for i,t in enumerate(predictions.tolist()):\n",
    "    print(raw_inputs[i])\n",
    "    print(\"\\tNegative Probability:\", t[0], \"Positivie Probability:\", t[1])\n",
    "# Recall the result when we used the pipeline at the top\n",
    "for p in sa(raw_inputs):\n",
    "    print(\"\\nPipeline result from above:\\n\",p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d82792-49e6-4316-afa6-bd8ba456a1b4",
   "metadata": {},
   "source": [
    "## So what did we do?\n",
    "![alt text](images/tokenizer.jpg)\n",
    "[Top of Page](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade26218-dac7-4078-a04b-0d78192f0a8b",
   "metadata": {},
   "source": [
    "## Your Assignment <a name=\"assignment\"> </a>\n",
    "\n",
    "This is pretty complex. Let this sink in a little bit....\n",
    "\n",
    "Any questions? This is about the limit of my knowledge about the inside workings of these simple NLP models.\n",
    "\n",
    "I think going deeper on this topic is not useful for us. Let's move on to more useful topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583ea139-c54e-4487-93c9-1cb20f0f46e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
