{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ab54ea4-b48e-45ec-a4dc-3a18abbd74d8",
   "metadata": {},
   "source": [
    "## LangChain: The Chat Prompt Template\n",
    "\n",
    "Use a more sophisticated Prompt Template in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b4210ac-3577-443e-8bb0-28183a803d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c5dce8-037b-4eeb-9736-5b8e40ad41ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "# Now you can access the environment variables\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "#\n",
    "# Not needed for this notebook\n",
    "# langchain_api_key = os.getenv('LANGCHAIN_API_KEY')\n",
    "# anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "# huggingface_api_key = os.getenv('HUGGINGFACE_API_KEY'\n",
    "#\n",
    "# You can always just assign your variable directly, just not good practice to expose your key in a notebook\n",
    "# anthropic_api_key='sk-ant-api03....._AAA' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9f3465-2c72-4a8f-ba0d-e11e938d2326",
   "metadata": {},
   "source": [
    "### Chat Prompt Template\n",
    "\n",
    "In LangChain, there are many types of Prompt Templates: <BR>\n",
    " - https://python.langchain.com/docs/modules/model_io/prompts/quick_start/\n",
    "\n",
    "Specifically, we are interested in a Prompt Template that works similar to the 'messages' format we used when calling the LLMs from the REST API. Remember the message format?\n",
    "\n",
    "```\n",
    "data = {\n",
    "    'model': 'claude-3-sonnet-20240229',\n",
    "    'max_tokens': 200,\n",
    "    'messages': [\n",
    "  {\"role\": \"user\", \"content\": \"I have an AI question, are you ready?\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"Hi, I'm Claude an AI assistant, so I know a lot about it. Please, please ask me anything about AI.\"},\n",
    "  {\"role\": \"user\", \"content\": \"Can you explain Artificial Neural Networks plain English, including an analogy?\"},\n",
    "]\n",
    "}\n",
    "```\n",
    "\n",
    "Chat Prompt Template Reference: https://python.langchain.com/docs/modules/model_io/prompts/quick_start/#chatprompttemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00efb4c0-9029-4866-81e5-d78b4b5c3643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptInput(user_input=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a new kind of prompt template\n",
    "#\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert in Large Language Models, especially working in Python. Your name is Clyde.\"),\n",
    "    (\"human\", \"Greetings Clyde, Please be ready to answer my questions about using LLMs with Python.\"),\n",
    "    (\"ai\", \"Yes, ready, willing and able.\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "# What do we need to provde to use it? \n",
    "template.input_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b2a727-94a3-48c9-a110-3a76f7a2eecf",
   "metadata": {},
   "source": [
    "Notice the type of messages that are possible: system, human, ai<BR>\n",
    "There are a couple more that we won't use here: FunctionMessage and ToolMessage <BR>\n",
    "See the full list here: https://python.langchain.com/docs/modules/model_io/chat/message_types/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f947f7b3-dbbf-45c3-b82a-90b1b9cf336a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are an expert in Large Language Models, especially working in Python. Your name is Clyde.'), HumanMessage(content='Greatings Clyde, Please be ready to answer my questions about using LLMs with Python.'), AIMessage(content='Yes, ready, willing and able.'), HumanMessage(content=\"\\nIn a HuggingFace tokenizer, please explain about how tokenized words related to a model's \\nvocabulary in a simple and clear way.\\n\")])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a string variable\n",
    "\n",
    "raw_input='''\n",
    "In a HuggingFace tokenizer, please explain about how tokenized words related to a model's \n",
    "vocabulary in a simple and clear way.\n",
    "'''\n",
    "\n",
    "# Recall, to create the template with your input, use the dictionary to pass your raw_input into the template\n",
    "# This doesn't call a model yet, as we have not defined it.\n",
    "template.invoke({\"user_input\": raw_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2c3d490-c223-4898-97c3-499cad0b2ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chain with this prompt, a model and a parser\n",
    "# ChatGPT model\n",
    "from langchain_openai import ChatOpenAI\n",
    "openai_llm = ChatOpenAI(model='gpt-3.5-turbo', api_key=openai_api_key)\n",
    "#\n",
    "# This parser takes the AIMessage reutrned form the LLM and converts it to a string\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "#\n",
    "# Create chain\n",
    "chain = template | openai_llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03d48616-c5cf-4fb3-a74d-59486cf1057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start the chain by using the invoke() method and our dictionary\n",
    "#\n",
    "response = chain.invoke({\"user_input\": raw_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c38cb2f6-018d-4ee2-a199-9816015c7ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a HuggingFace tokenizer, the tokenized words are converted into tokens that correspond to the model's vocabulary. Each word in the input text is broken down into smaller units called tokens, which are then mapped to indices in the model's vocabulary. This vocabulary consists of a fixed set of tokens that the model has been trained on.\n",
      "\n",
      "For example, the word \"cat\" might be tokenized into three tokens like ['ca', 't'] and then mapped to indices in the model's vocabulary, such as [145, 267]. When the model processes these tokens during training or inference, it looks up the corresponding embeddings for these indices in its vocabulary to perform computations.\n",
      "\n",
      "In summary, tokenized words are converted into tokens that are associated with specific indices in the model's vocabulary, allowing the model to understand and process the input text effectively.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4951e3d-01aa-4550-b68c-822e1b3976b4",
   "metadata": {},
   "source": [
    "### What we did\n",
    "1. Created a Chat Prompt Template (more sophisticated prompt template)\n",
    "2. Created a LLM\n",
    "3. Created a Output Parser\n",
    "4. Created a chain\n",
    "5. Used the chain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
