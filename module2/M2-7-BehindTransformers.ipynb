{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c73253e3-6499-4a3f-90bb-81065c2f19f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on full course\n",
    "# https://huggingface.co/learn/nlp-course/chapter0/1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95e4403-7ec9-4604-8ff5-ce4236725954",
   "metadata": {},
   "source": [
    "\n",
    "![alt text](images/tokenizer.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1826ecd4-5967-4630-9a6e-083186e3cb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-12 15:36:01.340200: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6af51771-2504-486a-a2a4-1972fa207551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "# Recall\n",
    "sa = pipeline(\"sentiment-analysis\",model = \"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ba1192d-3375-470c-82b8-bb8033488e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998805522918701},\n",
       " {'label': 'NEGATIVE', 'score': 0.9988584518432617}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the transformer just like last notebook\n",
    "raw_inputs = [\n",
    "    \"I really love this class!\",\n",
    "    \"I hate long, boring lectures.\",\n",
    "]\n",
    "sa(raw_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fa06f58-0e34-40fe-9664-92a45176194f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_name_or_path\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForSequenceClassification\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"finetuning_task\": \"sst-2\",\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"NEGATIVE\",\n",
       "    \"1\": \"POSITIVE\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"label2id\": {\n",
       "    \"NEGATIVE\": 0,\n",
       "    \"POSITIVE\": 1\n",
       "  },\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"transformers_version\": \"4.31.0\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look inside the Pipeline configuration\n",
    "sa.model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee73100a-95a9-472d-9921-32f6e000ff4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Use the same model\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "# Extract just the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b6de5ce-fd99-4699-acc9-a3c4ba3dbb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentences: tensor([[  101,  1045,  2428,  2293,  2023,  2465,   999,   102,     0],\n",
      "        [  101,  1045,  5223,  2146,  1010, 11771,  8921,  1012,   102]])\n",
      "Attention tensor: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# Use the tokenizer by passing it our 2 sentences\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\") # use pytorch tensors\n",
    "# Look at the sentences that are now tokenized\n",
    "print(\"Tokenized sentences:\", inputs['input_ids'])\n",
    "# Look at the attention mask, this tells the network which tokens to pay attention to \n",
    "print('Attention tensor:', inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb469853-6eec-431c-9ea2-285207d501ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "# Create just the model\n",
    "#model = AutoModel.from_pretrained(checkpoint, output_hidden_states=True,output_attentions=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, output_hidden_states=True,output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df2349c6-0af3-4458-aadb-3f968605e9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the type of the outputs? <class 'transformers.modeling_outputs.SequenceClassifierOutput'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inference the model. Send it the tokenized sentences\n",
    "outputs = model(**inputs) # Give the model all of our inputs (** just means unpack the dictionary)\n",
    "print(\"What is the type of the outputs?\", type(outputs))\n",
    "type(outputs.hidden_states)\n",
    "# Uncomment if you want to see the raw output from the model\n",
    "#outputs.hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea73100-bf12-49de-b002-a8f7c5821195",
   "metadata": {},
   "source": [
    "![alt text](images/heads.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fc79096-0d4d-47a6-99ae-99ed90dd4431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "tensor([[-4.3496,  4.6831],\n",
      "        [ 3.6689, -3.1053]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# This is our output, but it is still tokenized\n",
    "print(outputs.logits.shape)\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80c194ca-ef68-417d-839c-15c1b6347231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this model: {0: 'NEGATIVE', 1: 'POSITIVE'}\n",
      "Raw Probability Predictions in the tensors:\n",
      " tensor([[1.1943e-04, 9.9988e-01],\n",
      "        [9.9886e-01, 1.1415e-03]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      " ['I really love this class!', 'I hate long, boring lectures.'] \n",
      "\n",
      "Negative Probability: 0.00011942967830691487 Positivie Probability: 0.9998805522918701\n",
      "Negative Probability: 0.9988584518432617 Positivie Probability: 0.0011415336048230529\n"
     ]
    }
   ],
   "source": [
    "# Post process the tokens (Convert logits to probabilities)\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "# Print results\n",
    "print('In this model:', model.config.id2label)\n",
    "print('Raw Probability Predictions in the tensors:\\n', predictions)\n",
    "print('\\n', raw_inputs,'\\n')\n",
    "for t in predictions.tolist():\n",
    "    print(\"Negative Probability:\", t[0], \"Positivie Probability:\", t[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d82792-49e6-4316-afa6-bd8ba456a1b4",
   "metadata": {},
   "source": [
    "![alt text](images/tokenizer.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bec5cb-a58a-4604-8442-8560667229f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
