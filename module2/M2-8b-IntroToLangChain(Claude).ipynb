{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72bd44ff-2831-405d-84db-a0f3499e7010",
   "metadata": {},
   "source": [
    "## Still under development\n",
    "## Introduction to Chains using the LangChain Package\n",
    "The concept of an \"chain\" is an important tool for understanding and building generative AI applications. Chains are a key concept in LangChain, as they allow developers to create complex workflows by chaining together various components, such as language models, data sources, and processing steps.\n",
    "\n",
    "Understanding chains is crucial because they enable the creation of more sophisticated and powerful generative AI systems. Chains allow analysts to seamlessly integrate multiple functionalities, such as retrieving relevant information from external data sources, processing that information using language models, and generating coherent and contextual outputs.\n",
    "\n",
    "### Table of Contents <a name=\"top\"></a>\n",
    "1. [Create a LLM inside of LangChain](#llm)\n",
    "2. [Prompt Templates](#template)\n",
    "3. [Text Generation](#text-generation)\n",
    "4. [Question Answering](#question-answering)\n",
    "5. [Summarization](#summarization)\n",
    "6. [What's inside a pipeline?](#pipeline)\n",
    "7. [Your assignment](#assign)\n",
    "\n",
    "This content was adapted from: https://python.langchain.com/docs/get_started/quickstart/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b4210ac-3577-443e-8bb0-28183a803d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember, in order to run this notebook, you have to run the notebook M2-8a-Config_SM_image to configure the \n",
    "# SageMaker Docker image everytime you stop and restart the Jupyterlab Space.\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c5dce8-037b-4eeb-9736-5b8e40ad41ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "# Now you can access the environment variables\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "langchain_api_key = os.getenv('LANGCHAIN_API_KEY')\n",
    "huggingface_api_key = os.getenv('HUGGINGFACE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3175b433-1219-430a-8648-f0eb719fed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now import everything we will need\n",
    "# from langchain_anthropic import ChatAnthropic\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain import HuggingFaceHub\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e4d4c8-d272-4887-b98e-a1f54da1568c",
   "metadata": {},
   "source": [
    "## Create a chat model inside LangChain <a name=\"llm\"></a>\n",
    "Let's start with creating a chat model. We can use one from a wide selection, some of which will be familiar to you.\n",
    "\n",
    "https://python.langchain.com/docs/integrations/chat/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b441f0e-554e-4a2c-b896-daff1ef7ed08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'claude-3-sonnet-20240229',\n",
       " 'max_tokens': 1024,\n",
       " 'temperature': None,\n",
       " 'top_k': None,\n",
       " 'top_p': None,\n",
       " 'model_kwargs': {},\n",
       " 'streaming': False,\n",
       " 'max_retries': 2,\n",
       " 'default_request_timeout': None,\n",
       " '_type': 'anthropic-chat'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Docs: https://api.python.langchain.com/en/latest/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "claude_llm = ChatAnthropic(model=\"claude-3-sonnet-20240229\",api_key=anthropic_api_key)\n",
    "# Show some details about the model\n",
    "claude_llm.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a87ba9e9-4a9e-4af2-9625-6b773dcef4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple prompt\n",
    "raw_input = \"Please briefly explain the langchain package.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa273440-134c-4cb9-b7f9-e5e75450964d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the type of response?: <class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "# Invoke the model with our prompt\n",
    "response = claude_llm.invoke(\"Please briefly explain the langchain package.\", max_tokens = 100)\n",
    "print(\"What is the type of response?:\", type(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d40477c-8003-492f-a4b8-501977219dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangChain is a Python library that aims to make it easier to build applications with large language models (LLMs) like GPT-3, BLOOM, and others. It provides a set of abstractions and utilities that simplify the process of working with LLMs, allowing developers to focus on building their applications rather than dealing with the low-level details of interacting with the models.\\n\\nHere are some key features and capabilities of LangChain:\\n\\n1.', response_metadata={'id': 'msg_01MRW7XeNiEqaqpwaPaj1a6x', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'max_tokens', 'stop_sequence': None, 'usage': {'input_tokens': 15, 'output_tokens': 100}}, id='run-3a9080a5-d75a-481d-8ee1-3f3cc9b425d4-0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is the documentation:\n",
    "# https://api.python.langchain.com/en/latest/messages/langchain_core.messages.ai.AIMessage.html\n",
    "# Just dump the AIMessage to the screen\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "030fdef6-295d-40fa-98e8-b7b4a906e8e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': 'LangChain is a Python library that aims to make it easier to build applications with large language models (LLMs) like GPT-3, BLOOM, and others. It provides a set of abstractions and utilities that simplify the process of working with LLMs, allowing developers to focus on building their applications rather than dealing with the low-level details of interacting with the models.\\n\\nHere are some key features and capabilities of LangChain:\\n\\n1.',\n",
       " 'additional_kwargs': {},\n",
       " 'response_metadata': {'id': 'msg_01MRW7XeNiEqaqpwaPaj1a6x',\n",
       "  'model': 'claude-3-sonnet-20240229',\n",
       "  'stop_reason': 'max_tokens',\n",
       "  'stop_sequence': None,\n",
       "  'usage': {'input_tokens': 15, 'output_tokens': 100}},\n",
       " 'type': 'ai',\n",
       " 'name': None,\n",
       " 'id': 'run-3a9080a5-d75a-481d-8ee1-3f3cc9b425d4-0',\n",
       " 'example': False,\n",
       " 'tool_calls': [],\n",
       " 'invalid_tool_calls': []}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also get a Python dictionary\n",
    "response.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eeb75f-3140-4d2b-9702-e7107af6abb6",
   "metadata": {},
   "source": [
    "#### OpenAI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47118dc2-5676-41b4-a393-8defb172380e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'gpt-3.5-turbo',\n",
       " 'model': 'gpt-3.5-turbo',\n",
       " 'stream': False,\n",
       " 'n': 1,\n",
       " 'temperature': 0.7,\n",
       " '_type': 'openai-chat'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The OpenAI Chat is very similar\n",
    "# Documentation: https://python.langchain.com/docs/integrations/chat/openai/\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "openai_llm = ChatOpenAI(model='gpt-3.5-turbo', api_key=openai_api_key)\n",
    "openai_llm.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5dbd0da-755c-4260-92f1-316d50e05e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check out the type of the return from the model: <class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'content': 'The langchain package is a Python library for working with natural language processing (NLP) tasks, such as text classification, sentiment analysis, and named entity recognition. It provides a simple interface for building and training machine learning models for NLP tasks, as well as tools for preprocessing text data and evaluating model performance. The langchain package is designed to be easy to use and flexible, making it a useful tool for developers working on NLP projects.',\n",
       " 'additional_kwargs': {},\n",
       " 'response_metadata': {'token_usage': {'completion_tokens': 90,\n",
       "   'prompt_tokens': 15,\n",
       "   'total_tokens': 105},\n",
       "  'model_name': 'gpt-3.5-turbo',\n",
       "  'system_fingerprint': 'fp_c2295e73ad',\n",
       "  'finish_reason': 'stop',\n",
       "  'logprobs': None},\n",
       " 'type': 'ai',\n",
       " 'name': None,\n",
       " 'id': 'run-6822ff4a-22e8-4bb7-acb7-fdc116871ece-0',\n",
       " 'example': False,\n",
       " 'tool_calls': [],\n",
       " 'invalid_tool_calls': []}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Invoke the same way\n",
    "response = openai_llm.invoke(\"Please briefly explain the langchain package.\", max_tokens = 100)\n",
    "print('Check out the type of the return from the model:', type(response))\n",
    "# Look at some detail of the return\n",
    "response.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6c136e-bd77-420f-8258-fac917008240",
   "metadata": {},
   "source": [
    "#### Hugging Face Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cac8eb83-f720-4c36-9d38-3e9167bcbb8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'repo_id': 'HuggingFaceH4/zephyr-7b-beta',\n",
       " 'task': 'text-generation',\n",
       " 'model_kwargs': {'max_new_tokens': 512,\n",
       "  'top_k': 2,\n",
       "  'temperature': 0.1,\n",
       "  'repetition_penalty': 1.03},\n",
       " '_type': 'huggingface_hub'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also use HuggingFace models inside of LangChain\n",
    "# Documentation: https://python.langchain.com/docs/integrations/chat/huggingface/\n",
    "#\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "hf_llm = HuggingFaceHub(\n",
    "    huggingfacehub_api_token=huggingface_api_key,\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_k\": 2,\n",
    "        \"temperature\": 0.1,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "    },\n",
    ")\n",
    "hf_llm.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73411323-cf3a-42f6-9e1f-ded8496e084a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please briefly explain the langchain package. How does it help in building intelligent agents?\n",
      "\n",
      "LangChain is a Python library for building intelligent agents that can interact with various data sources, including text, audio, and video. It provides a framework for working with large-scale language models, such as GPT-3, and enables developers to build applications that can understand natural language, generate responses, and reason about information. LangChain's modular architecture allows for easy integration of different components, such as vector databases, caching, and dialog management, making it a versatile tool for building intelligent agents. Overall, LangChain helps in building intelligent agents by providing a set of tools and libraries for working with large-scale language models, enabling developers to build applications that can understand natural language, generate responses, and reason about information.\n"
     ]
    }
   ],
   "source": [
    "# Invoke the same way.\n",
    "response = hf_llm.invoke(\"Please briefly explain the langchain package.\", max_tokens=100)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "adfbf69a-76a4-4551-89f0-c3b2f7bf7f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have 3 models defined\n",
    "# claude_llm, openai_llm, hf_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f864674-7e70-47f5-ae97-ade6efe71a79",
   "metadata": {},
   "source": [
    "## Prompt Templates <a name=\"template\"></a>\n",
    "Prompt templates are predefined recipes for generating prompts for language models. A template may include instructions, few-shot examples, and specific context and questions appropriate for a given task. LangChain provides tooling to create and work with prompt templates. \n",
    "LangChain strives to create model agnostic templates to make it easy to reuse existing templates across different language models. Typically, language models expect the prompt to either be a string or else a list of chat messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0969663a-0882-4e15-ba5f-f0da16ca490d",
   "metadata": {},
   "source": [
    "### Simple Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b324866f-8a92-4b3c-a08a-b4419bdd3acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptInput(adjective=None, topic=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"adjective\",\"topic\"],\n",
    "    template=\"What is a {adjective} joke about {topic}?\",\n",
    ")\n",
    "\n",
    "prompt.input_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a41f087a-7adb-45ae-8615-ed3283b743a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is a funny joke about dogs?'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can use this by passing the parameters\n",
    "prompt.format(adjective=\"funny\",topic=\"dogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc6cdb32-a8d2-40a7-900e-a659e38f766d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='What is a strange joke about bears?')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# But we can also use the invoke() method and pass a dictionary. This is more common\n",
    "prompt.invoke({\"adjective\":\"strange\",\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4234d792-9ff6-429a-8425-0ece675eccd1",
   "metadata": {},
   "source": [
    "## Chains\n",
    "A LangChain chain is a fundamental concept in the LangChain framework that allows developers to create  workflows by chaining together various components, such as prompts, language models, parsers, data sources, and processing steps.\n",
    "\n",
    "Chains allow you to go beyond just a single API call to a language model and instead chain together multiple calls in a logical sequenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "92f1c635-4fe3-4db1-89bd-32cbda8ef231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.runnables.base.RunnableSequence"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a chain\n",
    "chain = prompt | claude_llm \n",
    "type(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "72581d21-5bca-415b-9613-b75ffb213f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Here's a happy, pun-filled joke about pumpkins:\\n\\nWhy did the pumpkin cross the road? To get to the other celery!\", response_metadata={'id': 'msg_015ewLVvffoCcpHF4sPzYHJS', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 17, 'output_tokens': 37}}, id='run-9c6fbe59-861b-43c9-9d7c-24251eb1e6d2-0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To use the chain, use the invoke() method\n",
    "chain.invoke({\"adjective\":\"happy\",\"topic\": \"pumpkins\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537335d1-4b96-466c-99da-971a48197848",
   "metadata": {},
   "source": [
    "## Parser\n",
    "Output parsers are responsible for taking the output of an LLM and transforming it to a more suitable format. This is very useful when you are using LLMs to generate any form of structured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e32ab829-26d8-4b0b-9515-fcd69bb9ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# This parser takes the AIMessage reutrned form the LLM and converts it to a string\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "236b507f-211d-4447-ba6e-d7b1419aa4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.runnables.base.RunnableSequence"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a chain with 3 steps\n",
    "chain = prompt | openai_llm | output_parser\n",
    "type(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "591b3382-0992-40b7-900f-c7fc2e77a547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of the resopnse: <class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Why did the chef get fired? Because he couldn't take the heat... or the criticism from Gordon Ramsay!\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the chain\n",
    "response = chain.invoke({\"adjective\":\"rude\",\"topic\": \"chefs\"})\n",
    "# Check out the response now. No longer an AIMessage datatype\n",
    "print(\"Type of the resopnse:\", type(response))\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad72379-067f-4233-82cf-b76b50705f7e",
   "metadata": {},
   "source": [
    "## Message Prompt Template\n",
    "Here is the message format that you have seen before in the API. This allows you to call the model with some more specific context or instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "51316354-5b4b-47ca-a25d-bcafa4f0a3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptInput(user_input=None)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful programming assistant. Your name is Clyde.\"),\n",
    "        (\"human\", \"Hello, I'm Kurt. Can you help me with my programming tasks?\"),\n",
    "        (\"ai\", \"Yes, I am ready, willing and able.\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "chat_prompt.input_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bd5db15f-220b-4eae-87ae-4ff8b19a9ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful programming assistant. Your name is Clyde.'),\n",
       " HumanMessage(content=\"Hello, I'm Kurt. Can you help me with my programming tasks?\"),\n",
       " AIMessage(content='Yes, I am ready, willing and able.'),\n",
       " HumanMessage(content='What is your name?')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = chat_prompt.format_messages( user_input=\"What is your name?\")\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f491743a-26bd-47aa-89ee-5066cc059b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Kurt, my name is Claude. It's nice to meet you! I'm an AI assistant created by Anthropic to help with all sorts of tasks, including programming. How can I assist you today?\", response_metadata={'id': 'msg_01NtWwphrypBUWHHEUrbfaqi', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 57, 'output_tokens': 46}}, id='run-b61b5eb2-b8a0-4e9c-869b-b85e0c4a74d1-0')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claude_llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "02b6af2d-fa82-4796-82ae-bc075b67d648",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_prompt | claude_llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "585aa896-b8c3-4206-a6f3-46e88a8c8f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your turn:  Help me create a for loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You entered:  Help me create a for loop.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful programming assistant. Your name is Clyde.'),\n",
       " HumanMessage(content=\"Hello, I'm Kurt. Can you help me with my programming tasks?\"),\n",
       " AIMessage(content='Yes, I am ready, willing and able.'),\n",
       " HumanMessage(content='Help me create a for loop.')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_input = input(\"Your turn: \")\n",
    "print(\"You entered: \", current_input)\n",
    "messages = chat_prompt.format_messages( user_input=current_input)\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "912bcd94-706e-480a-85df-39ecae337dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Kurt, certainly! I'd be happy to help you create a for loop. A for loop is used to iterate over a sequence (such as a list, tuple, or string) or other iterable objects. Here's a basic example in Python:\n",
      "\n",
      "```python\n",
      "# Iterating over a list\n",
      "fruits = [\"apple\", \"banana\", \"cherry\"]\n",
      "for fruit in fruits:\n",
      "    print(fruit)\n",
      "\n",
      "# Output:\n",
      "# apple\n",
      "# banana\n",
      "# cherry\n",
      "```\n",
      "\n",
      "In this example, the loop will iterate over each item in the `fruits` list, and for each iteration, the variable `fruit` will take the value of the current item. The loop body (indented code block) will be executed for each iteration.\n",
      "\n",
      "You can also use the `range()` function to iterate over a sequence of numbers:\n",
      "\n",
      "```python\n",
      "# Iterating over a range of numbers\n",
      "for i in range(5):\n",
      "    print(i)\n",
      "\n",
      "# Output:\n",
      "# 0\n",
      "# 1\n",
      "# 2\n",
      "# 3\n",
      "# 4\n",
      "```\n",
      "\n",
      "Here, the `range(5)` generates a sequence of numbers from 0 to 4 (up to, but not including, 5), and the loop iterates over each number, assigning it to the variable `i`.\n",
      "\n",
      "You can customize the start, stop, and step values of the `range()` function as needed:\n",
      "\n",
      "```python\n",
      "# Iterating over a custom range\n",
      "for j in range(2, 8, 2):\n",
      "    print(j)\n",
      "\n",
      "# Output:\n",
      "# 2\n",
      "# 4\n",
      "# 6\n",
      "```\n",
      "\n",
      "In this example, the loop will iterate over the numbers 2, 4, and 6.\n",
      "\n",
      "Let me know if you need further assistance or have a specific task in mind!\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "b44294b2-680e-4bc9-a00c-8297d04e1de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# chat_prompttemplate = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "#         (\"human\", \"Hello, how are you doing?\"),\n",
    "#         (\"ai\", \"I'm doing well, thanks!\"),\n",
    "#         (\"human\", \"{user_input}\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# messages = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "77ce3cad-8eba-4668-bd4c-84c49376b8c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful AI bot. Your name is Bob.'),\n",
       " HumanMessage(content='Hello, how are you doing?'),\n",
       " AIMessage(content=\"I'm doing well, thanks!\"),\n",
       " HumanMessage(content='What is your name?')]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a59ea8a7-343f-4932-a27a-ef7afb16d1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_template | openai_llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e2a32c56-5e4b-4274-8d74-f34460d293e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='My name is Clyde. How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 50, 'total_tokens': 62}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_d9767fc5b9', 'finish_reason': 'stop', 'logprobs': None}, id='run-a69b6f81-1872-40ad-b1d6-8c4558ea5e80-0')"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"name\":\"Clyde\",\"user_input\": \"What is your name?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb824774-0397-4701-9b95-dd908d47bf22",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "c20a72ce-387d-4d70-b0fe-f8613eb49fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chain\n",
    "chain = prompt | openai_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "7aa47e77-a8ea-41a3-ad94-35f3f603eae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the monkey like the banana?\\n\\nBecause it had appeal!', response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 15, 'total_tokens': 28}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_d9767fc5b9', 'finish_reason': 'stop', 'logprobs': None}, id='run-8fff555a-246c-4e22-81c6-98bcf9da26d7-0')"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can use the chain by using the invoke method and the dicitonary with the parameters\n",
    "chain.invoke({\"adjective\":\"silly\",\"topic\": \"monkeys\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d01ad3b-f5f7-42f2-b10d-2a40ee6bac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#myInput= {\"input\": \"how can langsmith help with testing?\"}\n",
    "#response = chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "response = chain.invoke(\"horses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79cb0e5-012e-4a51-baa5-676e9ad26907",
   "metadata": {},
   "source": [
    "## Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ad79e5c4-a68d-460f-a94f-09f8d1953843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "0edce4d2-a028-4e31-b3ec-5279314b7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | openai_llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "1b2a7cde-69a1-4c90-92ac-a9408d49c1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why did the chef get kicked out of the kitchen? \\n\\nBecause he couldn't take the heat and kept throwing pans at everyone!\""
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"adjective\":\"rude\",\"topic\": \"chefs\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcee7b0-bdf6-41c3-8fbb-d4365c6d3935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f3d1fd-557f-4f19-927e-50c96bfd1fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c10bdeb-5d08-492a-99f1-7e9da7fec2e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b0367b-e9cf-4442-9e40-d91bbc855dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fa667b-1c85-497a-8f4c-002b47d2a18d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ddbd2c-53f6-42de-bd43-fcc88313edd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ba4904-b8f3-4adb-81f2-08cb6c7be23e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9465b9d-d445-4237-8242-9bd094317cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3881ca90-ec07-4259-8545-826af7c96fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "72fd2047-1f04-4360-879a-fe870e72c6cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Chain.invoke() missing 1 required positional argument: 'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[144], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m chain \u001b[38;5;241m=\u001b[39m LLMChain(llm\u001b[38;5;241m=\u001b[39mopenai_llm, prompt\u001b[38;5;241m=\u001b[39mprompt)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#chain.run(\"podcast player\")\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#chain.invoke({\"input\": \"how can langsmith help with testing?\"})\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlawyers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Chain.invoke() missing 1 required positional argument: 'input'"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=openai_llm, prompt=prompt)\n",
    "#chain.run(\"podcast player\")\n",
    "#chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "chain.invoke(topic = \"lawyers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4df5fac-4f29-4466-8d4e-b71af29ea8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1229fb47-34de-45d6-8b0c-99d101d12ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc63d73b-6ce4-4745-ad01-7a613ddef534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136564c3-a57a-4040-b74b-fa5e78a6db50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a75dedcc-11af-42e0-ae28-912a04d1f418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a good name for a company that makes podcast player?\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")\n",
    "\n",
    "print(prompt.format(product=\"podcast player\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a3a091-cc49-4512-b0e0-32990a3a96a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9210a0-8c89-4d54-8242-f12f0d576a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378885be-dd9b-444a-980d-1452113c40ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f772b555-1063-46ba-bbe7-28de28627fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a funny joke about dogs.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "\n",
    "a = 'funny'\n",
    "c = 'dogs'\n",
    "\n",
    "prompt_template.format(adjective = a, content = c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "efed8c87-d91b-4f24-87ec-3ca4339297ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a silly joke about cats.'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'silly'\n",
    "c = 'cats'\n",
    "prompt_template.format(adjective= a, content= c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5c15fc-6034-4de4-874d-9c4e5a74646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "534c78a7-4a52-4288-8874-b4072718731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#myInput= {\"input\": \"how can langsmith help with testing?\"}\n",
    "#response = chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "response = chain.invoke(\"horses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4b93a740-90c1-465c-85ef-79a7a843821f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why couldn't the pony sing a lullaby? \\n\\nBecause he was a little hoarse!\""
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.dict()['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6022d889-0991-4475-8b24-093d3ed8b7ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b2ff57-31bb-44c2-bfe4-1b8da8c47567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "88feafe9-14ce-43be-9fe8-999df5ad52d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude: Why did the llama go to the barbershop? To get a llama cut!\n",
      "\n",
      "OpenAI: Why did the llama go to therapy? Because it had too much drama in its herd!\n",
      "\n",
      "HF Zepher: Tell me a odd joke about Llamas.\n",
      "I once knew an llama that could play the piano, but it was all uphill from there.\n",
      "How do you keep an llama in suspense?\n",
      "I'll tell you tomorrow.\n",
      "Why did the llama join the circus?\n",
      "To spit in the lion's face!\n",
      "Why did the llama get married?\n",
      "Because he found his cam-mo-le!\n",
      "What do you call a llama that can't be managed?\n",
      "A failama!\n",
      "Why did the llama wear sunglasses to the party?\n",
      "Because he heard it was a blinder!\n",
      "Why did the llama go to the doctor?\n",
      "Because he was feeling spitty!\n",
      "Why did the llama wear a lifejacket to the party?\n",
      "Because he heard it was a float!\n",
      "Why did the llama wear a hard hat to the party?\n",
      "Because he heard it was a construction!\n",
      "Why did the llama wear a helmet to the party?\n",
      "Because he heard it was a shell-ebration!\n",
      "Why did the llama wear a wetsuit to the party?\n",
      "Because he heard it was a wet celebration!\n",
      "Why did the llama wear a tutu to the party?\n",
      "Because he heard it was a bal-LEE-ma!\n",
      "Why did the llama wear a kilt to the party?\n",
      "Because he heard it was a Scottish celebration!\n",
      "Why did the llama wear a fez to the party?\n",
      "Because he heard it was a Turkish delight!\n",
      "Why did the llama wear a beret to the party?\n",
      "Because he heard it was a French celebration!\n",
      "Why did the llama wear a sombrero to the party?\n",
      "Because he heard it was a Mexican celebration!\n",
      "Why did the llama wear a cowboy hat to the party?\n",
      "Because he heard it was a western celebration!\n",
      "Why did the llama wear a graduation cap to the party?\n",
      "Because he heard it was a sheep-erence!\n",
      "Why did the llama wear a chef's hat to the party?\n",
      "Because he heard it was a cook-out!\n",
      "Why did the llama wear a clown suit to the party?\n",
      "Because he heard it was a circus celebration!\n",
      "Why did the llama wear a superhero costume to the party?\n",
      "Because he heard it was a crime-a-llama!\n",
      "Why did the llama wear a\n"
     ]
    }
   ],
   "source": [
    "# We can quickly call any of the models using the Prompt Template\n",
    "a = 'odd'\n",
    "c = 'Llamas'\n",
    "\n",
    "print('Claude:', claude_llm.invoke(prompt_template.format(adjective = a, content = c)).dict()['content'])\n",
    "print('\\nOpenAI:', openai_llm.invoke(prompt_template.format(adjective = a, content = c)).dict()['content'])\n",
    "print('\\nHF Zepher:', hf_llm.invoke(prompt_template.format(adjective = a, content = c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac484e49-006d-484e-9efc-d42cb059a23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain\n",
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a43779-1805-4849-b631-e13dad5abb82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f04ce33-94f3-4bce-b907-707f47d543ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c5e1ef-b3d6-4379-a926-64b88978a645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d732dfc-b4bf-4848-b308-52842a5f1b7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 3) (3512043507.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    (\"system\", \"You are world class technical documentation writer writing\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 3)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are world class technical documentation writer writing /\n",
    "    specifically for business master's degree students.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "facf93b3-b099-49e4-8b5b-9676d7bf0b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain\n",
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d18291c1-ffd5-4ca4-a9a4-d29c968bae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke\n",
    "myInput= {\"input\": \"how can langsmith help with testing?\"}\n",
    "#response = chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "response = chain.invoke(myInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e96a4172-fd4f-4ee7-9096-ef8cd764c16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langsmith can be a valuable tool for testing software applications and systems. Here are some ways Langsmith can assist with testing:\n",
      "\n",
      "1. **Test Case Generation**: Langsmith's natural language processing capabilities can be used to generate test cases from requirements documents, user stories, or other textual descriptions of the system's expected behavior. This can help automate the process of creating comprehensive test suites.\n",
      "\n",
      "2. **Test Data Generation**: Langsmith can analyze the application's data requirements and generate realistic test data, including edge cases and boundary conditions. This can be particularly useful for testing data-intensive applications or scenarios where manually creating test data is time-consuming or error-prone.\n",
      "\n",
      "3. **Test Automation**: Langsmith can be integrated with testing frameworks and tools to automate the execution of test cases. By understanding natural language instructions, Langsmith can translate high-level test scenarios into executable test scripts or code.\n",
      "\n",
      "4. **Test Reporting**: Langsmith can analyze test execution logs and generate human-readable reports summarizing the test results, highlighting failures, and providing insights into the root causes of issues. This can help streamline the testing process and improve communication among team members.\n",
      "\n",
      "5. **Test Maintenance**: As requirements or the application under test evolve, Langsmith can assist in updating and maintaining test cases and test data. By understanding the changes in natural language, Langsmith can suggest modifications to the existing test suite, reducing the effort required for manual updates.\n",
      "\n",
      "6. **Exploratory Testing**: Langsmith's natural language understanding capabilities can be leveraged for exploratory testing, where testers can describe test scenarios or use cases in natural language, and Langsmith can guide the testing process by suggesting relevant test steps or paths to explore.\n",
      "\n",
      "7. **Test Prioritization**: Langsmith can analyze the test suite, requirements, and historical data to prioritize test cases based on factors such as risk, complexity, or coverage. This can help optimize testing efforts and ensure that critical areas of the application are thoroughly tested.\n",
      "\n",
      "It's important to note that while Langsmith can be a powerful tool for testing, it should be used in conjunction with human expertise and domain knowledge. Testing is a complex process that requires critical thinking, creativity, and a deep understanding of the application and its requirements.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a6479f2-5fe4-484e-b920-77623b1f3590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1783eb1-1c9c-4f30-8a28-e309f2448392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defimne the 3-step-chain\n",
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba5e7ba3-a16c-4a84-8934-d675dc9ca44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the response is the result of the 3-steps\n",
    "response = chain.invoke(myInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37e812a0-0329-4238-8d5a-4bc1d4acff8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith can be a valuable tool for testing in several ways:\n",
      "\n",
      "1. **Test Case Generation**: LangSmith can be used to generate test cases automatically based on the requirements or specifications provided. It can analyze the input and generate a comprehensive set of test cases, covering various scenarios and edge cases.\n",
      "\n",
      "2. **Test Data Generation**: LangSmith can generate realistic and diverse test data for testing purposes. This can be particularly useful when testing with large datasets or when dealing with complex data structures.\n",
      "\n",
      "3. **Test Script Generation**: LangSmith can generate test scripts in various programming languages, such as Python, Java, or JavaScript. These scripts can be used to automate the testing process, reducing the need for manual testing and increasing efficiency.\n",
      "\n",
      "4. **Natural Language Test Case Description**: LangSmith can understand natural language descriptions of test cases and convert them into executable test scripts or test case specifications. This can be helpful when working with non-technical stakeholders or when translating requirements into testable scenarios.\n",
      "\n",
      "5. **Test Report Generation**: LangSmith can generate human-readable test reports based on the test results. These reports can provide detailed information about the tests executed, the results obtained, and any issues or failures encountered.\n",
      "\n",
      "6. **Test Maintenance**: LangSmith can assist in maintaining and updating test cases and scripts as the requirements or codebase evolves. It can analyze changes and suggest updates to the test suite, ensuring that the tests remain relevant and effective.\n",
      "\n",
      "7. **Test Automation Framework Integration**: LangSmith can be integrated with various test automation frameworks, such as Selenium, Appium, or Pytest, to enhance the testing capabilities and provide a more comprehensive testing solution.\n",
      "\n",
      "To effectively utilize LangSmith for testing, it is important to provide clear and accurate requirements, specifications, or existing test cases as input. Additionally, domain knowledge and expertise in testing practices are still necessary to ensure the quality and effectiveness of the generated test artifacts.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f384316-9aec-4a55-ac31-cf806c2ce901",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
