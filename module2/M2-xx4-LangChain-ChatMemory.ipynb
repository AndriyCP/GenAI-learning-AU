{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ab54ea4-b48e-45ec-a4dc-3a18abbd74d8",
   "metadata": {},
   "source": [
    "## Still in development, not ready for student use\n",
    "\n",
    "### LangChain: Using the Chat History\n",
    "\n",
    "https://www.pinecone.io/learn/series/langchain/langchain-conversational-memory/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b4210ac-3577-443e-8bb0-28183a803d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c5dce8-037b-4eeb-9736-5b8e40ad41ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "# Now you can access the environment variables\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "#\n",
    "# Not needed for this notebook\n",
    "# langchain_api_key = os.getenv('LANGCHAIN_API_KEY')\n",
    "# anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "# huggingface_api_key = os.getenv('HUGGINGFACE_API_KEY'\n",
    "#\n",
    "# You can always just assign your variable directly, just not good practice to expose your key in a notebook\n",
    "# anthropic_api_key='sk-ant-api03....._AAA' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddc9a4e7-3b18-48bc-a257-8cf2e2fe2656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.invoke(query)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d15f620-acd4-41e8-9697-4e349cb833c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 331 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'My interest here is to explore the potential of integrating Large Language Models with external knowledge',\n",
       " 'history': \"Human: Good morning AI!\\nAI: Good morning! How are you today?\\nHuman: Good morning AI!\\nAI: Good morning! How are you today?\\nHuman: Good morning AI!\\nAI: Good morning! How are you today?\\nHuman: My interest here is to explore the potential of integrating Large Language Models with external knowledge\\nAI: That's a fascinating topic! Large Language Models like GPT-3 have shown great potential in generating human-like text, but integrating them with external knowledge could greatly enhance their capabilities. By combining the language model's ability to generate natural language with external knowledge sources, such as databases, websites, or other sources of information, we could create AI systems that are even more intelligent and versatile. Do you have any specific ideas or goals in mind for this integration?\",\n",
       " 'response': \"That's a fascinating topic! Large Language Models like GPT-3 have shown great potential in generating human-like text, but integrating them with external knowledge could greatly enhance their capabilities. By combining the language model's ability to generate natural language with external knowledge sources, such as databases, websites, or other sources of information, we could create AI systems that are even more intelligent and versatile. Do you have any specific ideas or goals in mind for this integration?\"}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_buf, \n",
    "    \"My interest here is to explore the potential of integrating Large Language Models with external knowledge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60815f7-49a6-41d6-a607-e2913ce276a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tokens(\n",
    "    conversation_buf,\n",
    "    \"I just want to analyze the different possibilities. What can you think of?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d5a564-116b-4952-bc4b-dedc0faea07a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba687a6-d337-490b-8a7b-860d319eb550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a879105b-dd41-4886-a303-5d08c891802e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f652061e-2b39-4f34-90ba-8397be78eee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0bd013-ca5a-4597-a9bb-6ce7b7127a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b805095d-f630-4291-933c-7784ffaf7259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caaff1d-9d8a-4777-b59f-ee41cb118f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9390e1c9-e9ec-4d88-84a8-576b97e0466a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a84a0a-ecf9-4579-baea-8d5cbbb18fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dc1f2c-d762-4e00-b286-dd82da74ffd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d09bfd07-b804-4073-af71-04484a47a9f9",
   "metadata": {},
   "source": [
    "### Create a LLM with a Chat History enabled.\n",
    "\n",
    "To continue sending chat history to a large language model using LangChain in Python, you can leverage the memory functionality provided by LangChain. Here's how you can implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c22a5fc-42c4-4b12-8e3a-05f78ddd0101",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# first initialize the large language model\n",
    "openai_llm = ChatOpenAI(model='gpt-3.5-turbo', api_key=openai_api_key)\n",
    "# llm = OpenAI(\n",
    "# \ttemperature=0,\n",
    "# \topenai_api_key=openai_api_key,\n",
    "# \tmodel_name=\"text-davinci-003\"\n",
    "# )\n",
    "\n",
    "# now initialize the conversation chain\n",
    "conversation = ConversationChain(llm=openai_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eee63855-99ad-461a-b826-81802699749e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(conversation.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22c673ae-70b2-4a10-bae3-9fc3a9c47410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "\n",
    "conversation_buf = ConversationChain(\n",
    "    llm=openai_llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d244bab1-4278-4006-9708-0c8768f1e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = conversation_buf.invoke(\"Good morning AI!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "087e44db-c4bb-486a-8037-e9f9ce9073ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Good morning AI!',\n",
       " 'history': 'Human: Good morning AI!\\nAI: Good morning! How are you today?\\nHuman: Good morning AI!\\nAI: Good morning! How are you today?',\n",
       " 'response': 'Good morning! How are you today?'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee8180f-eb3f-4cb6-8af6-7fbaa45bc433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae697dc-cf6a-4cd0-ba6d-bdf8e29f58dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252950e3-e673-44b3-b76b-747d9b86eed1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a59ec5-ad1f-4ec9-8d58-d4440ea7a9f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d0980c-70da-4d5a-b04c-2e18d9e1560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the template\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert in Large Language Models, especially working in Python. Your name is Clyde.\"),\n",
    "    (\"human\", \"Greatings Clyde, Please be ready to answer my questions about using LLMs with Python.\"),\n",
    "    (\"ai\", \"Yes, ready, willing and able.\"),\n",
    "    (\"human\", \"{history}, {user_input}\"),\n",
    "])\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# raw_input = \"\"\"\n",
    "# You help everyone by answering questions, and improve your answers from previous answers in History.\n",
    "# Don't try to make up an answer, if you don't know, just say that you don't know.\n",
    "# Answer in the same language the question was asked.\n",
    "# Answer in a way that is easy to understand.\n",
    "# Do not say \"Based on the information you provided, ...\" or \"I think the answer is...\".\n",
    "# Just answer the question directly in detail.\n",
    "\n",
    "# History: {chat_history}\n",
    "\n",
    "# Question: {question}\n",
    "# Answer: \n",
    "# \"\"\"\n",
    "# template = PromptTemplate(\n",
    "#     template=raw_input,\n",
    "#     input_variables=[\"chat_history\", \"question\"]\n",
    "# )\n",
    "template.input_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a696675-5bbc-4f4a-ab51-3960ad009901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.llms import OpenAI\n",
    "#from langchain.chains import ConversationChain\n",
    "#from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad767e8e-16d8-4bea-a678-615c4b44893c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an object to hold the chatHistory\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "#chatHistory = ConversationBufferMemory(chat_memory='chatHistory')\n",
    "#chatHistory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "memory.save_context({\"input\": \"\"}, {\"output\": \"\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b1e97a-63b7-48a4-91f4-f0efdaa10d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#memory = ConversationBufferMemory(return_messages=True)\n",
    "#memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f134e8-3cb6-43c3-a77f-3beb45d16af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0, api_key=openai_api_key)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4467d89-bedb-4c1a-9964-ddf976e253cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7baf7f4-c84a-42cf-807a-08b1e9cccdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f05e617-b9fe-4da8-8d5e-f732c2d6c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Who is Socrates?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63541c8-976b-4fbc-8d2d-098d16d8563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e922669-4152-41a3-93ef-81c33cd6912b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Summarize out conversation so far.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c538ac-d376-49db-a1e8-34d68d6168b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "#chain = ConversationChain(llm=openai_llm, memory=chatHistory,prompt=template, verbose=True)\n",
    "conversation = ConversationChain(llm=openai_llm, prompt=template, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31bfdd6-b61d-4e72-8509-03eb3e216213",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730eedfa-7e56-4795-9693-44e1c853127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = conversation.predict(input=\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4933bb32-cf47-468e-b8cd-d2d38f03ed48",
   "metadata": {},
   "source": [
    "response = conversation.predict_and_parse(input=\"What is the weather like today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55d41d6-7921-4037-b01a-604ea3202ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5c47f3-8ae2-4887-bd44-b8132788fc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34ac553-8d17-479c-96fc-3f83a091c437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13283415-56b2-4d6c-affc-1fb8860e1e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7e6421-4e5f-44f9-a579-c15b82de7ada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d33bc09-ec8a-49d8-b169-4f6470898883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951de2e1-6cc4-4e23-9895-97b8ef5eb3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output parser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc34671-913b-43ff-a4b5-7b8766f6a852",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = template | openai_llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2014b72e-d101-45b2-9510-e6291dbe7b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the chain\n",
    "try:\n",
    "    response = chain.invoke({\"user_input\": raw_input})\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32db9316-e48c-479d-bc7c-cbf27eafa2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c061f453-18cf-423b-b574-80d1973a36d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6b23aa-3a70-4f71-8b60-cfecd21a3615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b8233c-4ca4-4045-a9fe-63510dcb9c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the chain\n",
    "raw_input=\"Clyde, please explain a LLM vector store and when I might use it.\"\n",
    "response = chain.invoke({\"user_input\": raw_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0c2724-a999-4492-a6b8-23f97db2eee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e12f19-240e-4466-a64e-952cd0bff1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to keep history?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7379db-c28f-496c-addf-d355412d172f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32be6144-cc69-49d9-a79e-8957464cfff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeef46a-53bd-4a64-bd4d-f1169363fcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1953c6-cf90-4928-8c34-cadc6e542ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bc063f-0ea2-46ac-ab5e-b15bc9c2c324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d373397-8a67-4865-95a4-5da82d4604ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c91a94e-ae08-45fc-9c2c-c55ebf016db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "openai_llm = ChatOpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d732dfc-b4bf-4848-b308-52842a5f1b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facf93b3-b099-49e4-8b5b-9676d7bf0b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain\n",
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18291c1-ffd5-4ca4-a9a4-d29c968bae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke\n",
    "myInput= {\"input\": \"how can langsmith help with testing?\"}\n",
    "#response = chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "response = chain.invoke(myInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96a4172-fd4f-4ee7-9096-ef8cd764c16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6479f2-5fe4-484e-b920-77623b1f3590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1783eb1-1c9c-4f30-8a28-e309f2448392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defimne the 3-step-chain\n",
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5e7ba3-a16c-4a84-8934-d675dc9ca44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the response is the result of the 3-steps\n",
    "response = chain.invoke(myInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e812a0-0329-4238-8d5a-4bc1d4acff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba941d7e-6d44-4018-8544-ae9e7b3a2ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
