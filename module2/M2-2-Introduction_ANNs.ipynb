{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ee4B4v5tAp1C"
   },
   "source": [
    "## A Simple Artificial Neural Network from Scratch with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my opinion, understanding artificial neural networks (ANNs) is pivotal for business analytics students to grasp the fundamentals of large language models and AI systems. Neural networks, which are computational models inspired by biological neural networks, form the backbone of these technologies. <BR>\n",
    "\n",
    "Understanding how neural networks learn by adjusting connection weights, interpreting their outputs based on neuron interactions, fine-tuning performance, recognizing limitations like data needs and biases, and staying up-to-date with advancements are crucial reasons to study them. This knowledge allows students to use AI tools properly, make informed choices, ensure responsible AI use, and keep pace with the rapidly evolving technology.<BR>\n",
    "\n",
    "All large language models are built using deep neural network architectures, typically transformer-based models with millions or billions of weights that get tuned on massive text datasets. The core principles of interconnected neuron layers, weight updates, and flexible model architectures from neural networks are applied at a grand scale to language tasks. Fundamentally, these large language models are a scaled-up application of neural network methods to the text domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w4cEhtf_Ap1E"
   },
   "source": [
    "In this tutorial we will implement a simple neural network from scratch using PyTorch. The idea of the tutorial is to teach you the basics of PyTorch and how it can be used to implement a neural network from scratch. I will go over some of the basic functionalities and concepts available in PyTorch that will allow you to build your own neural networks. \n",
    "\n",
    "This tutorial assumes you have prior knowledge of how a neural network works (maybe just from a YouTube video?). Even if you are not so sure, you will be okay. \n",
    "\n",
    "This tutorial was adapted from Elvis's post here: https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MP9ewMSlC7JU"
   },
   "source": [
    "\n",
    "The `torch` module provides all the necessary **tensor** operators you will need to implement your first neural network from scratch in PyTorch. That's right! In PyTorch everything is a Tensor, so this is the first thing you will need to get used to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bKmXKSQnAp1G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.0.0.post101\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: /opt/conda/lib/python3.10/site-packages\n",
      "Requires: filelock, jinja2, networkx, sympy, typing-extensions\n",
      "Required-by: accelerate, autogluon.multimodal, autogluon.timeseries, fastai, pytorch-lightning, pytorch-metric-learning, timm, torchmetrics, torchvision\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# The torch module provides all the necessary tensor operators you will need to implement your first neural network from scratch.\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# Display info about the Pytorch package\n",
    "%pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0D tensor: tensor([5])\n",
      "1D tensor: tensor([1, 2, 3, 4, 5])\n",
      "2D tensor:\n",
      " tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n"
     ]
    }
   ],
   "source": [
    "# Tensors are just multi-dimensional array or matrix of numbers\n",
    "# 0D tensor (scaler)\n",
    "print('0D tensor:', torch.tensor([5]))\n",
    "# Print 1D tensor\n",
    "print('1D tensor:', torch.tensor([1, 2, 3, 4, 5]))\n",
    "# Print a 3 rows by 2 column (2D) tensor\n",
    "print('2D tensor:\\n', torch.tensor([[1, 2], [3,4], [5,6]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1EWBBl1nAp1M"
   },
   "source": [
    "## Create an ANN to predict a student's grade based on hours studied and hours slept\n",
    "Data:<BR>\n",
    "Let's start by creating some sample data using the `torch.tensor` command.  We define types in PyTorch using the `dtype=torch.xxx` command. We'll use floats.\n",
    "\n",
    "`X(a,b)` represents the amount of hours studied and how much time students spent sleeping.<BR>\n",
    "`y` represent grades (0-100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fsAVbHnjAp1P"
   },
   "outputs": [],
   "source": [
    "# Data: hours [studied,slept]\n",
    "X = torch.tensor(([2, 9], [1, 5], [3, 6]), dtype=torch.float) # 3 X 2 tensor\n",
    "# labels: grade earned (0-100 scale)\n",
    "y = torch.tensor(([92], [100], [89]), dtype=torch.float) # 3 X 1 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "sfC-B1BEAp1W",
    "outputId": "d2ec7994-41ad-41fa-a69c-c0b7123ef7cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info about 'X': dimensions: 2 size: torch.Size([3, 2])\n",
      "Info about 'y': dimensions: 2 size: torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "# You can check the dimensions and size of the tensors\n",
    "print(\"Info about 'X': dimensions:\", X.dim(), \"size:\", X.size())\n",
    "print(\"Info about 'y': dimensions:\", y.dim(), \"size:\", y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zrND9MS9Ap1f"
   },
   "source": [
    "## Scaling\n",
    "\n",
    "As you likely know, scaling both the inputs and outputs can really help the performance of models. Below we are simply normalizing (scaling the data into (0-1) scale) the sample data. Notice that the `max` function returns both a tensor and the corresponding indices. So we use `_` as a throw-away variable to capture the indices which we won't use here.Our data is now in a very nice format our neural network will appreciate later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hlBvtfAmAp1i",
    "outputId": "23e1d24b-fa29-4173-f884-f44bc8a48cea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Tensor from the data: tensor([3., 9.])\n",
      "Scaled X: tensor([[0.6667, 1.0000],\n",
      "        [0.3333, 0.5556],\n",
      "        [1.0000, 0.6667]])\n",
      "Scaled y: tensor([[0.9200],\n",
      "        [1.0000],\n",
      "        [0.8900]])\n"
     ]
    }
   ],
   "source": [
    "# scale units: Normalize or min-max-scaling: everything to [0,1] interval\n",
    "X_max, _ = torch.max(X, 0) # Get the max of both hours studied and slept\n",
    "print(\"Max Tensor from the data:\", X_max)\n",
    "X = torch.div(X, X_max) # Scale by dividing tensor by max values possible\n",
    "print('Scaled X:', X)\n",
    "y = y / 100  # max test score is 100\n",
    "print('Scaled y:', y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xRvMSpEFAp1n"
   },
   "source": [
    "## Model (Computation Graph)\n",
    "Once the data has been prepared, all you need to do now is to define your model. At the end of the day we are constructing a computation graph, which is used to dictate how data should flow and what type of operations are performed on this information. \n",
    "\n",
    "For illustration purposes, we are building the following neural network or computation graph:\n",
    "\n",
    "![alt text](images/fig1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C7pDC5SfAp1p"
   },
   "outputs": [],
   "source": [
    "# Define a class with shape of the NN and its functions\n",
    "class Neural_Network(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Neural_Network, self).__init__()\n",
    "        # Define the shape of the model: \n",
    "        self.inputSize = 2 # Number of input layers\n",
    "        self.outputSize = 1 # Number of output layers\n",
    "        self.hiddenSize = 3 # Number of hidden layers\n",
    "        \n",
    "        # Initialize the weights with random numbers\n",
    "        self.W1 = torch.randn(self.inputSize, self.hiddenSize) # 3 X 2 tensor\n",
    "        self.W2 = torch.randn(self.hiddenSize, self.outputSize) # 3 X 1 tensor\n",
    "    # Feed forward function\n",
    "    def forward(self, X):\n",
    "        self.z = torch.matmul(X, self.W1) # 3 X 3 \".dot\" does not broadcast in PyTorch\n",
    "        self.z2 = self.sigmoid(self.z) # activation function\n",
    "        self.z3 = torch.matmul(self.z2, self.W2)\n",
    "        o = self.sigmoid(self.z3) # final activation function\n",
    "        return o\n",
    "    # Activation function        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + torch.exp(-s))\n",
    "    # Derivative of the activation function\n",
    "    def sigmoidPrime(self, s):\n",
    "        # derivative of sigmoid\n",
    "        return s * (1 - s)\n",
    "    # Backward feed function\n",
    "    def backward(self, X, y, o):\n",
    "        self.o_error = y - o # error in output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o) # derivative of sig to error\n",
    "        self.z2_error = torch.matmul(self.o_delta, torch.t(self.W2))\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2)\n",
    "        self.W1 += torch.matmul(torch.t(X), self.z2_delta)\n",
    "        self.W2 += torch.matmul(torch.t(self.z2), self.o_delta)\n",
    "    # Training function        \n",
    "    def train(self, X, y):\n",
    "        # forward + backward pass for training\n",
    "        o = self.forward(X)\n",
    "        self.backward(X, y, o)\n",
    "    # Save the weights    \n",
    "    def saveWeights(self, model):\n",
    "        # we will use the PyTorch internal storage functions\n",
    "        torch.save(model, \"NN\")\n",
    "        # you can reload model with all the weights and so forth with:\n",
    "        # torch.load(\"NN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qm5gimnyAp1s"
   },
   "source": [
    "For the purpose of this tutorial, we are not going to be talking the details of ANN implementation. Mostly, I just want you to get a gist of what it takes to build a neural network from scratch using PyTorch. Let's break down the model which was declared via the class above. \n",
    "\n",
    "### Class Header\n",
    "First, we defined our model via a class because that is the recommended way to build the computation graph. The class header contains the name of the class `Neural Network` and the parameter `nn.Module` which basically indicates that we are defining our own neural network. \n",
    "\n",
    "```python\n",
    "class Neural_Network(nn.Module):\n",
    "```\n",
    "\n",
    "### Initialization\n",
    "The next step is to define the initializations ( `def __init__(self,)`) that will be performed upon creating an instance of the customized neural network. You can declare the parameters of your model here, but typically, you would declare the structure of your network in this section -- the size of the hidden layers and so forth. Since we are building the neural network from scratch, we explicitly declared the size of the weights matrices: one that stores the parameters from the input to hidden layer; and one that stores the parameter from the hidden to output layer. Both weight matrices are initialized with values randomly chosen from a normal distribution via `torch.randn(...)`. Note that we are not using bias just to keep things as simple as possible.  \n",
    "\n",
    "```python\n",
    "def __init__(self, ):\n",
    "    super(Neural_Network, self).__init__()\n",
    "    # Define the shape of the model: \n",
    "    self.inputSize = 2\n",
    "    self.outputSize = 1\n",
    "    self.hiddenSize = 3\n",
    "\n",
    "    # weights\n",
    "    self.W1 = torch.randn(self.inputSize, self.hiddenSize) # 3 X 2 tensor\n",
    "    self.W2 = torch.randn(self.hiddenSize, self.outputSize) # 3 X 1 tensor\n",
    "```\n",
    "\n",
    "### The Forward Function\n",
    "The `forward` function is where all the magic happens (see below). This is where the data enters and is fed into the computation graph (i.e., the neural network structure we have built). Since we are building a simple neural network with one hidden layer, our forward function looks very simple:\n",
    "\n",
    "```python\n",
    "def forward(self, X):\n",
    "    self.z = torch.matmul(X, self.W1) \n",
    "    self.z2 = self.sigmoid(self.z) # activation function\n",
    "    self.z3 = torch.matmul(self.z2, self.W2)\n",
    "    o = self.sigmoid(self.z3) # final activation function\n",
    "    return o\n",
    "```\n",
    "\n",
    "The `forward` function above takes the input `X`and then performs a matrix multiplication (`torch.matmul(...)`) with the first weight matrix `self.W1`. Then the result is applied an activation function, `sigmoid`. The resulting matrix of the activation is then multiplied with the second weight matrix `self.W2`. Then another activation if performed, which renders the output of the neural network or computation graph. The process I described above is simply what's known as a `feedforward pass`. In order for the weights to optimize when training, we need a backpropagation algorithm. \n",
    "\n",
    "### The Backward Function\n",
    "The `backward` function contains the backpropagation algorithm, where the goal is to essentially minimize the loss with respect to our weights. In other words, the weights need to be updated in such  a way that the loss decreases while the neural network is training (well, that is what we hope for). All this magic is possible with the gradient descent algorithm which is declared in the `backward` function. Take a minute or two to inspect what is happening in the code below:\n",
    "\n",
    "```python\n",
    "def backward(self, X, y, o):\n",
    "    self.o_error = y - o # error in output\n",
    "    self.o_delta = self.o_error * self.sigmoidPrime(o) \n",
    "    self.z2_error = torch.matmul(self.o_delta, torch.t(self.W2))\n",
    "    self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2)\n",
    "    self.W1 += torch.matmul(torch.t(X), self.z2_delta)\n",
    "    self.W2 += torch.matmul(torch.t(self.z2), self.o_delta)\n",
    "```\n",
    "\n",
    "Notice that we are performing a lot of matrix multiplications along with the transpose operations via the `torch.matmul(...)` and `torch.t(...)` operations, respectively. The rest is simply gradient descent -- there is nothing to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9t26Dr5zAp1u"
   },
   "source": [
    "## Training\n",
    "All that is left now is to train the neural network. First we create an instance of the computation graph we have just built:\n",
    "\n",
    "```python\n",
    "NN = Neural_Network()\n",
    "```\n",
    "\n",
    "Then we train the model for `1000` rounds. Notice that in PyTorch `NN(X)` automatically calls the `forward` function so there is no need to explicitly call `NN.forward(X)`. \n",
    "\n",
    "After we have obtained the predicted output for every round of training, we compute the loss, with the following code:\n",
    "\n",
    "```python\n",
    "torch.mean((y - NN(X))**2).detach().item()\n",
    "```\n",
    "\n",
    "The next step is to start the training (foward + backward) via `NN.train(X, y)`. After we have trained the neural network, we can store the model and output the predicted value of the single instance we declared in the beginning, `xPredicted`.  \n",
    "\n",
    "Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17156
    },
    "colab_type": "code",
    "id": "9sTddOpLAp1w",
    "outputId": "a02d2b93-34da-4068-f1f2-843f1e30abf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 Loss: 0.11654549837112427\n",
      "#999 Loss: 0.0018166033551096916\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an instance of the class we defined above\n",
    "NN = Neural_Network()\n",
    "# Train using a loop\n",
    "loss_values = [] # keep track of loss function values each loop\n",
    "for i in range(1000):  # trains the NN 1,000 times\n",
    "    loss = torch.mean((y - NN(X))**2).detach().item() # Compute the mean sum squared loss value\n",
    "    loss_values.append(loss)  # Store the loss value\n",
    "    # Uncomment the next line if you want to see the loss function decrease as it learns\n",
    "    #print(\"#\" + str(i) + \" Loss: \" + str(loss))\n",
    "    # Just print the first and last values of the loss \n",
    "    if i == 0 or i == 999:\n",
    "        print (\"#\" + str(i) + \" Loss: \" + str(loss))\n",
    "    NN.train(X, y)\n",
    "# After training, plot the loss values as they improve\n",
    "plt.plot(loss_values)\n",
    "plt.title('Loss values')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "# We've done the work of training, so let's save our weights.\n",
    "NN.saveWeights(NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss decreases rapidly, but then slows. This means that the neural network has learned how to predict the grade based on the hours studied/slept. Don't lose the context of the overly simplified model and lack of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "To predict a new student grade using our trained ANN, just feed a scaled tensor into the forward() method of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a predict funciton to take care of the details\n",
    "#\n",
    "def predict(t): # Accept a tensor\n",
    "    scaled_t = torch.div(t, X_max) # Scale it the same as the training data\n",
    "    prediction = NN.forward(scaled_t) # predict by calling the forward function from the trained NN\n",
    "    return prediction.item() # Return the raw decimal value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serious Nick's grade: 91.6%\n",
      "Smart Ryan's grade: 94.7%\n",
      "Dance Floor Darren's grade: 93.3%\n"
     ]
    }
   ],
   "source": [
    "# Nick is a serious student, he studies 3 hours and sleeps only 4 hours\n",
    "Nick = predict(torch.tensor(([3,4]), dtype=torch.float))\n",
    "print(\"Serious Nick's grade:\", \"{:.1%}\".format(Nick))\n",
    "#\n",
    "# Ryan is naturally pretty smart, doesn't study much, but he sleeps well [.5,8]\n",
    "Ryan = predict(torch.tensor(([.5,8]), dtype = torch.float))\n",
    "print(\"Smart Ryan's grade:\", \"{:.1%}\".format(Ryan))\n",
    "#\n",
    "# Darren is a poor student and serious partier [.1,.1]\n",
    "Darren =  predict(torch.tensor(([.1,.1]), dtype=torch.float))\n",
    "print(\"Dance Floor Darren's grade:\", \"{:.1%}\".format(Darren))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L9nBzkgdbjcA"
   },
   "source": [
    " That's it. Congratulations! You have just learned how to create and train a aritifical neural network from scratch using PyTorch. Remember, ANNs are the core of LLMs and AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zcms4BCySKXj"
   },
   "source": [
    "## References:\n",
    "- [Elvis' Post](https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0)\n",
    "- [PyTorch nn. Modules](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-custom-nn-modules)\n",
    "- [Build a Neural Network with Numpy](https://enlight.nyc/neural-network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "nn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
