{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edf86a47-b70a-498a-8dbc-de917d47fa84",
   "metadata": {},
   "source": [
    "### Retrival Augmentented Generation\n",
    "Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts.\n",
    "\n",
    "#### Vector embeddings\n",
    "Let's begin with the fundamental building blocks of how retrieval works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7347c523-adac-43c8-b651-acc6281afd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain_cohere -q\n",
    "%pip install spacy -q\n",
    "#ignore error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13d4086-c974-496a-99e1-2d5584e0755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now you need to run this in a terminal window\n",
    "# python -m spacy download en_core_web_md\n",
    "# now restart your kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4a4fed-79a7-4616-96b5-813c6f5bcc9a",
   "metadata": {},
   "source": [
    "Standard imports for the libraires we will be using in this notebook.  Try to keep your imports in the first cell so this can this code can more easliy be converted into a python program later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1513731f-ab9f-4e74-9249-e8ff08dfd433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import pyarrow\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain_community.embeddings.spacy_embeddings import SpacyEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "\n",
    "# Create the AWS client for the Bedrock runtime with boto3\n",
    "aws_client = boto3.client(service_name=\"bedrock-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceec844-3eee-4ce4-993d-d72ab9dd8560",
   "metadata": {},
   "source": [
    "#### Lets define functions that will use various embedding models so we can generate vector embeddings\n",
    "Spacey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1b6024-dda4-4dd3-836d-5830b472d8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spacy_vector_embedding(text):\n",
    "    embedder = SpacyEmbeddings(model_name=\"en_core_web_md\")\n",
    "    query_embedding = embedder.embed_query(text)\n",
    "\n",
    "    return(np.array(query_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf40345-ad82-4d53-bfa6-19360a03c4de",
   "metadata": {},
   "source": [
    "Huggingface Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb9c64d-f1a7-4123-b197-05d485553761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_huggingface_vector_embedding(text):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"bert-base-uncased\")\n",
    "    embedded_texts = embeddings.embed_query(text_data)\n",
    "    return(embedded_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35157a12-af69-4d79-8a9f-d7229eef4bec",
   "metadata": {},
   "source": [
    "Cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0667bf3-01ef-4493-8cbc-744413c0d457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send in an array size of one and only return the 0th element\n",
    "def generate_cohere_vector_embedding(text_data):\n",
    "    input_type = \"clustering\"\n",
    "    truncate = \"NONE\" # optional\n",
    "    model_id = \"cohere.embed-english-v3\" # or \"cohere.embed-multilingual-v3\"\n",
    "    \n",
    "    # Create the JSON payload for the request\n",
    "    json_params = {\n",
    "            'texts': [text_data],\n",
    "            'truncate': truncate, \n",
    "            \"input_type\": input_type\n",
    "        }\n",
    "    json_body = json.dumps(json_params)\n",
    "    params = {'body': json_body, 'modelId': model_id,}\n",
    "    \n",
    "    # Invoke the model and print the response\n",
    "    result = aws_client.invoke_model(**params)\n",
    "    response = json.loads(result['body'].read().decode())\n",
    "    return(np.array(response['embeddings'][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef0f33d-429d-4636-9082-d7bbb9850790",
   "metadata": {},
   "source": [
    "Amazon Titan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5794765f-0b32-451e-a145-6a7b4ce785e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate a dense vector using Amazon Titan with LangChain\n",
    "def generate_titan_vector_embedding(text):\n",
    "    #create an Amazon Titan Text Embeddings client\n",
    "    embeddings_client = BedrockEmbeddings(region_name=\"us-west-2\") \n",
    "\n",
    "    #Invoke the model\n",
    "    embedding = embeddings_client.embed_query(text)\n",
    "    return(np.array(embedding))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff40e7e-c880-4dd1-b7ad-70f8a12efe2a",
   "metadata": {},
   "source": [
    "This is the mathmatical formula to calcuate cosine similarity between 2 vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8367a7-c786-41fc-ad05-d53b5633d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "    return similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7230a6-86c4-4950-a793-6bae2bb4925d",
   "metadata": {},
   "source": [
    "#### Persistance of embeddings for later retrieval\n",
    " In order to do semantic search and retrieve relevant content we need to store that content for later use.  We can store the embedding in several different persistence technologies.  To start simply let's store the data in memory using pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6763ad27-f4b8-4f6a-8200-d067332ea4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_value(value):\n",
    "    value_str = str(value)\n",
    "    cleaned_value = ''.join(char for char in value_str if char.isalnum() or char.isspace())\n",
    "    return cleaned_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14383a5-eec2-40e7-9c61-ccadb437d46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_string_size(x, max_chars=2048):\n",
    "    # Check if the input is a string\n",
    "    if isinstance(x, str):\n",
    "        return x[:max_chars]\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d11db83-cffa-4ebc-8f41-5392bb92cc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean abstract text\n",
    "df = pd.read_csv('INSERT YOUR COOL DATASET HERE')\n",
    "df['abstract'] = df['abstract'].apply(clean_value)\n",
    "# for cohere we need to limit the string size to 2048\n",
    "#df['abstract'] = df['abstract'].apply(limit_string_size)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8695452-a792-4a38-81b3-dd6d554d60ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings using spaCy\n",
    "# Add a new column 'embedded_abstract' by applying the function to an existing column\n",
    "# This step takes a while so I did it for you and saved the output as pickle\n",
    "#df['embedded_abstract'] = df['abstract'].apply(generate_spacy_vector_embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08975b47-f547-4930-a7e4-42876c00e7e9",
   "metadata": {},
   "source": [
    "### Retrieval from embedded sources\n",
    "Now that we have a dataframe with embedded content of interest, we can use semantic similarity to retrieve the right data to feed to an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b25889-1d6a-4c2f-bd19-5d117cff23f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's setup a query that a used might ask\n",
    "query = \"What is the latest research for broken ribs in children\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b0949d-dad2-4fa3-b6cc-a11c5a61fc09",
   "metadata": {},
   "source": [
    "#### Titan Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02d48ef-2c13-4f4f-9389-76b60de4ffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = pd.read_pickle('data/embedded_df.pkl')\n",
    "# Let's search our records for a good semantic search\n",
    "query_vector = generate_titan_vector_embedding(query)\n",
    "\n",
    "results = []\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in dft.iterrows():\n",
    "    # Extract the value from the specified column\n",
    "    article_embedding = row['embedded_abstract']\n",
    "    results.append((index, cosine_similarity(article_embedding, query_vector)))\n",
    "    #print (index, value)\n",
    "\n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "i = 0\n",
    "# Print the sorted data\n",
    "print(\"Here are a few articles that may match your interest:\")\n",
    "for item in results:\n",
    "    article_title = dft.iloc[item[0]]['title']\n",
    "    print(f\"Abtract: '{article_title}' with a cosine match of: {item[1]}\")\n",
    "    i=i+1\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630d870d-041d-497b-8e0c-b28862191a02",
   "metadata": {},
   "source": [
    "### Assignment\n",
    "Using a dataset of your choosing try and retrieve relevant context for given user input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b233ecf-3bd3-4ad9-97d1-257cd990522b",
   "metadata": {},
   "source": [
    "1) Use kaggle or other source of data that has information that you want to use for context to an LLM call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c0265-c074-4d73-b9bd-d6c4ce741a24",
   "metadata": {},
   "source": [
    "Upload your dataset to the data folder and briefly describe why you selected this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3672543f-e090-48a3-98f5-cb5f9458b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your response here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92fd8cb-cca5-4da5-b621-dab0bf7b23bd",
   "metadata": {},
   "source": [
    "2. Data ingestion -> In reviewing your datasource what needs to be done?  Is there data cleansing involved is there chunking required? Why or why not?\n",
    "\n",
    "Perform and data cleansinfg or chunking here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ed48764-300c-41e3-a7a9-ff9bc28ffe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c21483-c621-4429-893e-39e04029e4ee",
   "metadata": {},
   "source": [
    "3. Load the data into a dataframe and embed the data for retrieval.\n",
    "\n",
    "Which embedding model do you pick and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e56bb26-a6b9-48b5-8617-b7399267587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d315e34-610e-4e21-a495-16b4245293cb",
   "metadata": {},
   "source": [
    "4. Now test your retrieval by using a few test queries to sample the data.\n",
    "\n",
    "How did you determine your retrieval threshold (i.e. What cosine similarty did you pick, did it remain consistent for different queries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73e71054-d534-48f1-b29e-f1f578debdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db35c825-8c82-4a33-90bb-2c0a30e23c67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
