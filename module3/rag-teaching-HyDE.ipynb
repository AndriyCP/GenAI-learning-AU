{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edf86a47-b70a-498a-8dbc-de917d47fa84",
   "metadata": {},
   "source": [
    "### Retrival Augmentented Generation Day 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7347c523-adac-43c8-b651-acc6281afd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain_cohere -q\n",
    "%pip install spacy -q\n",
    "%pip install psycopg2 -q\n",
    "%pip install python-dotenv -q\n",
    "#ignore error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13d4086-c974-496a-99e1-2d5584e0755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now you need to run this in a terminal window\n",
    "# python -m spacy download en_core_web_md\n",
    "# now restart your kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4a4fed-79a7-4616-96b5-813c6f5bcc9a",
   "metadata": {},
   "source": [
    "Standard imports for the libraires we will be using in this notebook.  Try to keep your imports in the first cell so this can this code can more easliy be converted into a python program later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1513731f-ab9f-4e74-9249-e8ff08dfd433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pyarrow\n",
    "import traceback\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "import dbconnection\n",
    "import psycopg2\n",
    "from psycopg2 import OperationalError\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# Create the AWS client for the Bedrock runtime with boto3\n",
    "aws_client = boto3.client(service_name=\"bedrock-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceec844-3eee-4ce4-993d-d72ab9dd8560",
   "metadata": {},
   "source": [
    "#### Lets define functions that will use various embedding models so we can generate vector embeddings\n",
    "Spacey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1b6024-dda4-4dd3-836d-5830b472d8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spacy_vector_embedding(text):\n",
    "    embedder = SpacyEmbeddings(model_name=\"en_core_web_md\")\n",
    "    query_embedding = embedder.embed_query(text)\n",
    "\n",
    "    return(np.array(query_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35157a12-af69-4d79-8a9f-d7229eef4bec",
   "metadata": {},
   "source": [
    "Cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0667bf3-01ef-4493-8cbc-744413c0d457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send in an array size of one and only return the 0th element\n",
    "def generate_cohere_vector_embedding(text_data):\n",
    "    input_type = \"clustering\"\n",
    "    truncate = \"NONE\" # optional\n",
    "    model_id = \"cohere.embed-english-v3\" # or \"cohere.embed-multilingual-v3\"\n",
    "    \n",
    "    # Create the JSON payload for the request\n",
    "    json_params = {\n",
    "            'texts': [text_data],\n",
    "            'truncate': truncate, \n",
    "            \"input_type\": input_type\n",
    "        }\n",
    "    json_body = json.dumps(json_params)\n",
    "    params = {'body': json_body, 'modelId': model_id,}\n",
    "    \n",
    "    # Invoke the model and print the response\n",
    "    result = aws_client.invoke_model(**params)\n",
    "    response = json.loads(result['body'].read().decode())\n",
    "    return(np.array(response['embeddings'][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef0f33d-429d-4636-9082-d7bbb9850790",
   "metadata": {},
   "source": [
    "Amazon Titan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5794765f-0b32-451e-a145-6a7b4ce785e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate a dense vector using Amazon Titan with LangChain\n",
    "def generate_titan_vector_embedding(text):\n",
    "    #create an Amazon Titan Text Embeddings client\n",
    "    embeddings_client = BedrockEmbeddings(region_name=\"us-west-2\") \n",
    "\n",
    "    #Invoke the model\n",
    "    embedding = embeddings_client.embed_query(text)\n",
    "    return(np.array(embedding))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08b3826-e751-4f03-b564-e2eb2d3ff00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate a dense vector using Amazon Titan without using a np.array as a return value\n",
    "def generate_vector_embedding(text):\n",
    "    #create an Amazon Titan Text Embeddings client\n",
    "    embeddings_client = BedrockEmbeddings(region_name=\"us-west-2\") \n",
    "\n",
    "    #Invoke the model\n",
    "    embedding = embeddings_client.embed_query(text)\n",
    "    #Note pgvector does not want a np.array as out manual method\n",
    "    return(embedding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff40e7e-c880-4dd1-b7ad-70f8a12efe2a",
   "metadata": {},
   "source": [
    "This is the mathmatical formula to calcuate cosine similarity between 2 vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8367a7-c786-41fc-ad05-d53b5633d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "    return similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6763ad27-f4b8-4f6a-8200-d067332ea4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_value(value):\n",
    "    value_str = str(value)\n",
    "    cleaned_value = ''.join(char for char in value_str if char.isalnum() or char.isspace())\n",
    "    return cleaned_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14383a5-eec2-40e7-9c61-ccadb437d46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_string_size(x, max_chars=2048):\n",
    "    # Check if the input is a string\n",
    "    if isinstance(x, str):\n",
    "        return x[:max_chars]\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c74692-d25d-48ae-8fde-04ba53188a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_values(list_stuff: list, num_items: int) -> None:\n",
    "    i=0\n",
    "    for item in list_stuff:\n",
    "        i=i+1\n",
    "        if i>num_items:\n",
    "            return None\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d11db83-cffa-4ebc-8f41-5392bb92cc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean abstract text\n",
    "#df = pd.read_csv('data/latest_research_articles.csv')\n",
    "#df['abstract'] = df['abstract'].apply(clean_value)\n",
    "\n",
    "#df\n",
    "dft = pd.read_pickle('data/embedded_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea212b3-6d8f-441b-9671-35a9c1346474",
   "metadata": {},
   "source": [
    "### Advanced Retrieval Techniques\n",
    "#### HyDE\n",
    "A technique that optimizes semantic matching requires better semantic context.  What if we generated a document from the query that better match our stored document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33147a2d-fc4c-4b20-83c5-34ee32426933",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieval from embedded sources\n",
    "#Now that we have a dataframe with embedded content of interest, we can use semantic similarity to retrieve the right data to feed to an LLM\n",
    "\n",
    "# Given the following query let's generate context that more closely matches the embedded data\n",
    "query = \"What is the latest research for broken ribs in children\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c85c16-1ef1-424d-9167-09166e696e49",
   "metadata": {},
   "source": [
    "#### Calling the LLM with Python\n",
    "Before we embed the vector with the query let's transform the query into a fake article.  This article will likely have a larger semantic overlap than the original smaller question. Using Bedrock we will now call Anthropic Claude Sonnet to generate a fictitous article.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48e81ca-c394-49ab-b5f0-cbaab5f7aeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate HyDE context\n",
    "\n",
    "def generate_hyde_response(query_phrase):\n",
    "    model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "    # Each model will named parameters which will likely be different depending on the providor\n",
    "    model_kwargs =  { \n",
    "        \"max_tokens\": 400, # This is the maximum output tokens you want the model to use\n",
    "        \"temperature\": 1,  # Temperature controls the randomness and creativity of the generated text.\n",
    "        \"top_k\": 250,      # Top-k parameter determines the number of highest probability next word choices the model should conside\n",
    "        \"top_p\": 0.9,      # Top-p sampling considers the cumulative probability distribution of the next word choices and sets a probability threshold\n",
    "        \"stop_sequences\": [\"\\n\\nHuman\"],\n",
    "    }\n",
    "    # LangChain tooling\n",
    "    model = BedrockChat(\n",
    "        client=aws_client,\n",
    "        model_id=model_id,\n",
    "        model_kwargs=model_kwargs,\n",
    "    )\n",
    "    \n",
    "    human_prompt = \"Given the following question \\n {query} can you please generate a paragraph of text that answers the question. Be sure to use scientific \\\n",
    "                    medical terminology. Please just include the paragraph in your response.\"\n",
    "    # Uses the messaging method which is required for all Claude 3 calls\n",
    "    messages = [\n",
    "        (\"system\", \"You are a helpful assistant\"),\n",
    "        (\"human\", human_prompt),\n",
    "    ]\n",
    "    try:\n",
    "        prompt = ChatPromptTemplate.from_messages(messages)\n",
    "        # LangChain at work\n",
    "        chain = prompt | model | StrOutputParser()\n",
    "\n",
    "\n",
    "        # Send the message content to Claude using Bedrock and get the response\n",
    "        start_time = time.time()  # Start timing\n",
    "        # Call Bedrock\n",
    "        response = chain.invoke({\"query\": query_phrase})\n",
    "        end_time = time.time()  # End timing\n",
    "        print(\"Claude call took :\", end_time - start_time)  # Calculate execution time\n",
    "\n",
    "        return(response)\n",
    "    except Exception as e:\n",
    "        exc_type, exc_value, exc_traceback = traceback.sys.exc_info()\n",
    "        line_number = exc_traceback.tb_lineno\n",
    "        print(f\"Errort: {exc_type}{exc_value}{exc_traceback} on {line_number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f80486-0d15-4500-88f2-c4eb93550ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_hyde_response(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b0949d-dad2-4fa3-b6cc-a11c5a61fc09",
   "metadata": {},
   "source": [
    "#### Titan Embeddings - SAME No HyDE\n",
    "Let's review our cosine similarties with using our query term embedded as is to find articles that are simantically similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1325dd-7635-4017-871b-c12bfa26faaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's search our records for a good semantic search\n",
    "# First we will embbed our search term into a vector so that we can mathmatically compare them\n",
    "query_vector = generate_titan_vector_embedding(query)\n",
    "\n",
    "# This is a tuple of the article index and the cosine similarity score\n",
    "# We will use this to sort the 'closest match'\n",
    "results = []\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in dft.iterrows():\n",
    "    # Extract the value from the specified column\n",
    "    article_embedding = row['embedded_abstract']\n",
    "    results.append((index, cosine_similarity(article_embedding, query_vector)))\n",
    "    #print (index, value)\n",
    "\n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "i = 0\n",
    "# Print the sorted data\n",
    "print(\"Here are a few articles that may match your interest:\")\n",
    "for item in results:\n",
    "    article_title = dft.iloc[item[0]]['title']\n",
    "    print(f\"Abtract: '{article_title}' with a cosine match of: {item[1]}\")\n",
    "    i=i+1\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effb9abd-19f9-4e88-8b16-91fc57a440da",
   "metadata": {},
   "source": [
    "We can see the difference between our first result and our next best result.\n",
    "\n",
    "Now let's compare our cosine scores with HyDE . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02d48ef-2c13-4f4f-9389-76b60de4ffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's search our records for a good semantic search\n",
    "# First we will embbed our search term into a vector so that we can mathmatically compare them\n",
    "query_vector = generate_titan_vector_embedding(generate_hyde_response(query))\n",
    "\n",
    "# This is a tuple of the article index and the cosine similarity score\n",
    "# We will use this to sort the 'closest match'\n",
    "results = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in dft.iterrows():\n",
    "    # Extract the value from the specified column\n",
    "    article_embedding = row['embedded_abstract']\n",
    "    # store the results in our result tuple\n",
    "    results.append((index, cosine_similarity(article_embedding, query_vector)))\n",
    "    #print (index, value)\n",
    "\n",
    "# Sort the results into highest match as the first record\n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "i = 0\n",
    "# Print the sorted data\n",
    "print(\"Here are a few articles that may match your interest:\")\n",
    "for item in results:\n",
    "    # Use the index from the Original dataframe to extract values of interest\n",
    "    article_title = dft.iloc[item[0]]['title']\n",
    "    print(f\"Abtract: '{article_title}' with a cosine match of: {item[1]}\")\n",
    "    i=i+1\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34db1008-1a93-4fca-9210-484fddeace0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
