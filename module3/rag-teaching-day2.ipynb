{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edf86a47-b70a-498a-8dbc-de917d47fa84",
   "metadata": {},
   "source": [
    "### Retrival Augmentented Generation Day 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7347c523-adac-43c8-b651-acc6281afd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "botocore 1.34.51 requires urllib3<2.1,>=1.25.4; python_version >= \"3.10\", but you have urllib3 2.2.1 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.1.4 which is incompatible.\n",
      "transformers 4.31.0 requires tokenizers!=0.11.3,<0.14,>=0.11.1, but you have tokenizers 0.19.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_cohere -q\n",
    "%pip install spacy -q\n",
    "#ignore error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13d4086-c974-496a-99e1-2d5584e0755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now you need to run this in a terminal window\n",
    "# python -m spacy download en_core_web_md\n",
    "# now restart your kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4a4fed-79a7-4616-96b5-813c6f5bcc9a",
   "metadata": {},
   "source": [
    "Standard imports for the libraires we will be using in this notebook.  Try to keep your imports in the first cell so this can this code can more easliy be converted into a python program later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1513731f-ab9f-4e74-9249-e8ff08dfd433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pyarrow\n",
    "import traceback\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# Create the AWS client for the Bedrock runtime with boto3\n",
    "aws_client = boto3.client(service_name=\"bedrock-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceec844-3eee-4ce4-993d-d72ab9dd8560",
   "metadata": {},
   "source": [
    "#### Lets define functions that will use various embedding models so we can generate vector embeddings\n",
    "Spacey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd1b6024-dda4-4dd3-836d-5830b472d8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spacy_vector_embedding(text):\n",
    "    embedder = SpacyEmbeddings(model_name=\"en_core_web_md\")\n",
    "    query_embedding = embedder.embed_query(text)\n",
    "\n",
    "    return(np.array(query_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf40345-ad82-4d53-bfa6-19360a03c4de",
   "metadata": {},
   "source": [
    "Huggingface Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfb9c64d-f1a7-4123-b197-05d485553761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_huggingface_vector_embedding(text):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"bert-base-uncased\")\n",
    "    embedded_texts = embeddings.embed_query(text_data)\n",
    "    return(embedded_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35157a12-af69-4d79-8a9f-d7229eef4bec",
   "metadata": {},
   "source": [
    "Cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0667bf3-01ef-4493-8cbc-744413c0d457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send in an array size of one and only return the 0th element\n",
    "def generate_cohere_vector_embedding(text_data):\n",
    "    input_type = \"clustering\"\n",
    "    truncate = \"NONE\" # optional\n",
    "    model_id = \"cohere.embed-english-v3\" # or \"cohere.embed-multilingual-v3\"\n",
    "    \n",
    "    # Create the JSON payload for the request\n",
    "    json_params = {\n",
    "            'texts': [text_data],\n",
    "            'truncate': truncate, \n",
    "            \"input_type\": input_type\n",
    "        }\n",
    "    json_body = json.dumps(json_params)\n",
    "    params = {'body': json_body, 'modelId': model_id,}\n",
    "    \n",
    "    # Invoke the model and print the response\n",
    "    result = aws_client.invoke_model(**params)\n",
    "    response = json.loads(result['body'].read().decode())\n",
    "    return(np.array(response['embeddings'][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef0f33d-429d-4636-9082-d7bbb9850790",
   "metadata": {},
   "source": [
    "Amazon Titan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5794765f-0b32-451e-a145-6a7b4ce785e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate a dense vector using Amazon Titan with LangChain\n",
    "def generate_titan_vector_embedding(text):\n",
    "    #create an Amazon Titan Text Embeddings client\n",
    "    embeddings_client = BedrockEmbeddings(region_name=\"us-west-2\") \n",
    "\n",
    "    #Invoke the model\n",
    "    embedding = embeddings_client.embed_query(text)\n",
    "    return(np.array(embedding))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff40e7e-c880-4dd1-b7ad-70f8a12efe2a",
   "metadata": {},
   "source": [
    "This is the mathmatical formula to calcuate cosine similarity between 2 vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e8367a7-c786-41fc-ad05-d53b5633d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "    return similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4047ec8c-4391-4504-b444-783e00f64f08",
   "metadata": {},
   "source": [
    "Now let's generate vectors for simple words and compare how we can compute cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7230a6-86c4-4950-a793-6bae2bb4925d",
   "metadata": {},
   "source": [
    "#### Persistance of embeddings for later retrieval\n",
    " In order to do semantic search and retrieve relevant content we need to store that content for later use.  We can store the embedding in several different persistence technologies.  To start simply let's store the data in memory using pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6763ad27-f4b8-4f6a-8200-d067332ea4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_value(value):\n",
    "    value_str = str(value)\n",
    "    cleaned_value = ''.join(char for char in value_str if char.isalnum() or char.isspace())\n",
    "    return cleaned_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b14383a5-eec2-40e7-9c61-ccadb437d46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_string_size(x, max_chars=2048):\n",
    "    # Check if the input is a string\n",
    "    if isinstance(x, str):\n",
    "        return x[:max_chars]\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d11db83-cffa-4ebc-8f41-5392bb92cc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean abstract text\n",
    "#df = pd.read_csv('data/latest_research_articles.csv')\n",
    "#df['abstract'] = df['abstract'].apply(clean_value)\n",
    "# for cohere we need to limit the string size to 2048\n",
    "#df['abstract'] = df['abstract'].apply(limit_string_size)\n",
    "#df\n",
    "dft = pd.read_pickle('data/embedded_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea212b3-6d8f-441b-9671-35a9c1346474",
   "metadata": {},
   "source": [
    "### Advanced Retrieval Techniques\n",
    "#### HyDE\n",
    "A technique that optimizes semantic matching requires better semantic context.  What if we generated a document from the query hat better match our stored document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33147a2d-fc4c-4b20-83c5-34ee32426933",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieval from embedded sources\n",
    "#Now that we have a dataframe with embedded content of interest, we can use semantic similarity to retrieve the right data to feed to an LLM\n",
    "\n",
    "# Given the following query let's generate context that more closely matches the embeded data\n",
    "query = \"What is the latest research for broken ribs in children\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f48e81ca-c394-49ab-b5f0-cbaab5f7aeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate HyDE context\n",
    "\n",
    "def generate_hyde_response(query_phrase):\n",
    "    model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "    model_kwargs =  { \n",
    "        \"max_tokens\": 300,\n",
    "        \"temperature\": 1,\n",
    "        \"top_k\": 250,\n",
    "        \"top_p\": 0.9,\n",
    "        \"stop_sequences\": [\"\\n\\nHuman\"],\n",
    "    }\n",
    "\n",
    "    model = BedrockChat(\n",
    "        client=aws_client,\n",
    "        model_id=model_id,\n",
    "        model_kwargs=model_kwargs,\n",
    "    )\n",
    "\n",
    "    human_prompt = \"Given the following question \\n {query} can you please generate a paragraph of text that answers the question. Be sure to use scientific \\\n",
    "                    medical terminology. Please just include the paragraph in your response.\"\n",
    "    messages = [\n",
    "        (\"system\", \"You are a helpful assistant\"),\n",
    "        (\"human\", human_prompt),\n",
    "    ]\n",
    "    try:\n",
    "        prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "        chain = prompt | model | StrOutputParser()\n",
    "\n",
    "\n",
    "        # Send the message content to Claude using Bedrock and get the response\n",
    "        start_time = time.time()  # Start timing\n",
    "        # Call Bedrock\n",
    "        response = chain.invoke({\"query\": query_phrase})\n",
    "        end_time = time.time()  # End timing\n",
    "        print(\"Claude call took :\", end_time - start_time)  # Calculate execution time\n",
    "\n",
    "        return(response)\n",
    "    except Exception as e:\n",
    "        exc_type, exc_value, exc_traceback = traceback.sys.exc_info()\n",
    "        line_number = exc_traceback.tb_lineno\n",
    "        print(f\"Errort: {exc_type}{exc_value}{exc_traceback} on {line_number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53f80486-0d15-4500-88f2-c4eb93550ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude call took : 4.258257150650024\n",
      "Pediatric rib fractures, although relatively uncommon, require prompt evaluation and management to mitigate potential complications. Recent research has focused on identifying risk factors and optimizing treatment modalities. Thorough clinical assessment, including detailed history and physical examination, coupled with appropriate imaging modalities, such as chest radiography and computed tomography scans, are crucial for accurate diagnosis and exclusion of associated injuries. Conservative management, including adequate analgesia, respiratory support, and close monitoring, is often sufficient for uncomplicated cases. However, surgical intervention may be warranted in cases of flail chest, significant displacement, or associated injuries. Multidisciplinary collaboration between pediatric emergency physicians, radiologists, and surgeons is essential for comprehensive care and favorable outcomes in pediatric rib fracture management.\n"
     ]
    }
   ],
   "source": [
    "print(generate_hyde_response(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b0949d-dad2-4fa3-b6cc-a11c5a61fc09",
   "metadata": {},
   "source": [
    "#### Titan Embeddings - SAME No HyDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd1325dd-7635-4017-871b-c12bfa26faaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are a few articles that may match your interest:\n",
      "Abtract: 'High sensitivity methods for automated rib fracture detection in pediatric radiographs' with a cosine match of: 0.46759500523692665\n",
      "Abtract: 'Magnetic resonance imaging based finite element modelling of the proximal femur: a short-term in vivo precision study' with a cosine match of: 0.23057253235100217\n",
      "Abtract: 'On the crashworthiness analysis of bio-inspired DNA tubes' with a cosine match of: 0.21674978237483608\n",
      "Abtract: 'Reproduction of forearm rotation dynamic using intensity-based biplane 2D–3D registration matching method' with a cosine match of: 0.19907903320363604\n",
      "Abtract: 'Propagation of extended fractures by local nucleation and rapid transverse expansion of crack-front distortion' with a cosine match of: 0.19056102046718387\n"
     ]
    }
   ],
   "source": [
    "# Let's search our records for a good semantic search\n",
    "query_vector = generate_titan_vector_embedding(query)\n",
    "\n",
    "results = []\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in dft.iterrows():\n",
    "    # Extract the value from the specified column\n",
    "    article_embedding = row['embedded_abstract']\n",
    "    results.append((index, cosine_similarity(article_embedding, query_vector)))\n",
    "    #print (index, value)\n",
    "\n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "i = 0\n",
    "# Print the sorted data\n",
    "print(\"Here are a few articles that may match your interest:\")\n",
    "for item in results:\n",
    "    article_title = dft.iloc[item[0]]['title']\n",
    "    print(f\"Abtract: '{article_title}' with a cosine match of: {item[1]}\")\n",
    "    i=i+1\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b02d48ef-2c13-4f4f-9389-76b60de4ffa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude call took : 6.943073749542236\n",
      "Here are a few articles that may match your interest:\n",
      "Abtract: 'High sensitivity methods for automated rib fracture detection in pediatric radiographs' with a cosine match of: 0.5205147419233009\n",
      "Abtract: 'AI co-pilot bronchoscope robot' with a cosine match of: 0.24361273445341824\n",
      "Abtract: 'Non-invasive biomarkers for detecting progression toward hypovolemic cardiovascular instability in a lower body negative pressure model' with a cosine match of: 0.2429005756019053\n",
      "Abtract: 'Reproduction of forearm rotation dynamic using intensity-based biplane 2D–3D registration matching method' with a cosine match of: 0.23163465008048445\n",
      "Abtract: 'Magnetic resonance imaging based finite element modelling of the proximal femur: a short-term in vivo precision study' with a cosine match of: 0.22144321379030013\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Let's search our records for a good semantic search\n",
    "query_vector = generate_titan_vector_embedding(generate_hyde_response(query))\n",
    "# This is a tuple of the article index and the cosine similarity score\n",
    "results = []\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in dft.iterrows():\n",
    "    # Extract the value from the specified column\n",
    "    article_embedding = row['embedded_abstract']\n",
    "    results.append((index, cosine_similarity(article_embedding, query_vector)))\n",
    "    #print (index, value)\n",
    "\n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "i = 0\n",
    "# Print the sorted data\n",
    "print(\"Here are a few articles that may match your interest:\")\n",
    "for item in results:\n",
    "    # Use the index from the Original dataframe to extract values of interest\n",
    "    article_title = dft.iloc[item[0]]['title']\n",
    "    print(f\"Abtract: '{article_title}' with a cosine match of: {item[1]}\")\n",
    "    i=i+1\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34db1008-1a93-4fca-9210-484fddeace0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0b6c4e8-e7f1-40a0-b34a-84e5d67a86e5",
   "metadata": {},
   "source": [
    "#### Differentiating the retrievals and ranking\n",
    " How can we evaluate if the cosine similarities are different enough?  Thresholds aren't consistent enough.  What about Z-Score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05f073f8-9e1d-4222-9e68-dc5ea4800b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_zscores(cosine_scores):\n",
    "    zscores = []\n",
    "    # Calculate the mean of the sample points\n",
    "    mean = np.mean(cosine_scores)\n",
    "    # Calculate the standard deviation of the sample points\n",
    "    std_deviation = np.std(cosine_scores, ddof=1)  # ddof=1 for sample standard deviation\n",
    "    # Calculate the z-scores for each sample point\n",
    "    z_scores = [(x - mean) / std_deviation for x in cosine_scores]\n",
    "\n",
    "    return z_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1322753b-eefc-472d-874a-a71c10633981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abtract: 'High sensitivity methods for automated rib fracture detection in pediatric radiographs' with a cosine match of: 0.46759500523692665 and Z-score of: 10.649061266665777\n"
     ]
    }
   ],
   "source": [
    "# grab the cosine_scores from the results tuple\n",
    "cosine_scores = [item[1] for item in results]\n",
    "z_scores = calculate_zscores(cosine_scores)\n",
    "#save the articles that were a good match\n",
    "articles = \"\"\n",
    "i=0\n",
    "first_z_score = z_scores[0]\n",
    "for item in results:\n",
    "    # Use the index from the Original dataframe to extract values of interest\n",
    "    article_title = dft.iloc[item[0]]['title']\n",
    "    abstract = dft.iloc[item[0]]['abstract']\n",
    "    # If the highest Z score is 2x this record then likey not as significant\n",
    "    if(first_z_score/2)<z_scores[i]:\n",
    "        print(f\"Abtract: '{article_title}' with a cosine match of: {item[1]} and Z-score of: {z_scores[i]}\")\n",
    "        articles = articles + \" \" + abstract\n",
    "    i=i+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f268e1a0-5ac3-4f96-b838-a816980344e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's take the records that are greater that 1/2 the top Z-score and send those to the LLM for an answer\n",
    "def best_answer(data, question):\n",
    "    model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "    model_kwargs =  { \n",
    "        \"max_tokens\": 2048,\n",
    "        \"temperature\": 0.0,\n",
    "        \"top_k\": 250,\n",
    "        \"top_p\": 0.9,\n",
    "        \"stop_sequences\": [\"\\n\\nHuman\"],\n",
    "    }\n",
    "\n",
    "    model = BedrockChat(\n",
    "        client=aws_client,\n",
    "        model_id=model_id,\n",
    "        model_kwargs=model_kwargs,\n",
    "    )\n",
    "\n",
    "    human_prompt = \"You are to answer the question using the data in the following information.  Do not make up your answer, only use \\\n",
    "                    supporting data from the article, If you don't have enough data simply respond, I don't have enough information to answer that question. \\\n",
    "                    given the following article data {data} can you please give a concise answer to the following question. {question}\"\n",
    "    messages = [\n",
    "        (\"system\", \"You are a helpful assistant that can answer quesitons based on news articles you have been given.\"),\n",
    "        (\"human\", human_prompt),\n",
    "    ]\n",
    "    try:\n",
    "        prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "        chain = prompt | model | StrOutputParser()\n",
    "\n",
    "        # Chain Invoke\n",
    "        \n",
    "    \n",
    "        # Send the message content to Claude using Bedrock and get the response\n",
    "        start_time = time.time()  # Start timing\n",
    "        # Call Bedrock\n",
    "        response = chain.invoke({\"data\": data,\"question\": question})\n",
    "        end_time = time.time()  # End timing\n",
    "        #print(\"Claude call took :\", end_time - start_time)  # Calculate execution time\n",
    "\n",
    "        return(response)\n",
    "    except Exception as e:\n",
    "        exc_type, exc_value, exc_traceback = traceback.sys.exc_info()\n",
    "        line_number = exc_traceback.tb_lineno\n",
    "\n",
    "        return f\"ERROR generating good answer: {exc_type}{exc_value}{exc_traceback} on {line_number}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "472a37eb-70a1-4059-ac56-4a17a6322db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the best answer I could find about  What is the latest research for broken ribs in children\n",
      "Based on the information provided in the article, the latest research focuses on improving the detection and localization of rib fractures in pediatric chest radiographs using computer vision and machine learning techniques. Specifically:\n",
      "\n",
      "- The research implemented convolutional neural network (CNN) architectures like RetinaNet and YOLOv5, along with an \"avalanche decision\" scheme to dynamically adjust detection thresholds, to increase the sensitivity (recall) in detecting rib fractures.\n",
      "\n",
      "- Multiple image preprocessing techniques and model ensembling methods were used to further enhance detection performance.\n",
      "\n",
      "- Using a dataset of 1,109 pediatric chest radiographs manually labeled by radiologists, the best performing model achieved an F2 score of 0.725, approaching the expert inter-reader performance of 0.732.\n",
      "\n",
      "- The goal was to develop methods that can identify all rib fractures to aid radiologists in interpreting pediatric chest radiographs, as rib fractures are highly indicative of non-accidental trauma in young children and can be challenging to detect.\n",
      "\n",
      "In summary, the latest research focuses on applying advanced computer vision and machine learning techniques to improve the sensitivity of detecting and localizing rib fractures in children's chest X-rays.\n"
     ]
    }
   ],
   "source": [
    "print(\"Here is the best answer I could find about \", query)\n",
    "print(best_answer(articles, query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec7d51e-1dda-432a-b8a1-4fb659ba1d0d",
   "metadata": {},
   "source": [
    "#### Chunking\n",
    "What do we do if our text we want is larger than the embedding model we plan to use? Breaking the text into logical chunks that we can still combine later for retrieval.  Let's embed data from a large text document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2c8b2df-f7ce-4942-b267-97e488f29d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Load your large text document\n",
    "with open(\"data/raft.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Create a text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,  # Adjust the chunk size as needed\n",
    "    chunk_overlap=200,  # Adjust the overlap between chunks as needed\n",
    ")\n",
    "\n",
    "# Split the text into chunks\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "# create a dataframe with new chunk raw text\n",
    "chunk_df = pd.DataFrame({'raw_text': chunks})\n",
    "\n",
    "chunk_df['text_embed'] = chunk_df['raw_text'].apply(generate_titan_vector_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e50d5ca1-f710-4e0e-8312-a147f4378e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the best answer I could find about  What is new about the RAFT technique, compared to regular RAG\n",
      "Based on the information provided in the article, the key novelties of the RAFT (Retrieval-Augmented Fine-Tuning) technique compared to regular Retrieval-Augmented Generation (RAG) are:\n",
      "\n",
      "1. RAFT fine-tunes the language model on domain-specific data, training it to generate answers from provided documents and questions. This allows the model to learn and adapt to the appropriate answering style for that domain.\n",
      "\n",
      "2. During training, RAFT includes some examples where the oracle (ground truth) documents are removed, forcing the model to rely more on memorization rather than just extracting from context. This makes the model more robust to imperfect document retrieval.\n",
      "\n",
      "3. RAFT trains the model to generate answers using a \"chain-of-thought\" approach, providing detailed reasoning that cites relevant quotes from the context documents. This enhances the model's ability to process context and extract useful information.\n",
      "\n",
      "4. RAFT incorporates \"distractor\" documents during training that are irrelevant to the question, mimicking imperfect retrieval scenarios. This trains the model to distinguish relevant from irrelevant information.\n",
      "\n",
      "In summary, while regular RAG focuses on retrieving and generating from documents at inference time, RAFT adapts the language model during training to better process domain documents, handle imperfect retrieval, and provide reasoned, evidence-based answers.\n"
     ]
    }
   ],
   "source": [
    "# Let's search our records for a good semantic search\n",
    "query = \"What is new about the RAFT technique, compared to regular RAG\"\n",
    "\n",
    "query_vector = generate_titan_vector_embedding(query)\n",
    "# This is a tuple of the article index and the cosine similarity score\n",
    "results = []\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in chunk_df.iterrows():\n",
    "    # Extract the value from the specified column\n",
    "    article_embedding = row['text_embed']\n",
    "    results.append((index, cosine_similarity(article_embedding, query_vector)))\n",
    "    #print (index, cosine_similarity(article_embedding, query_vector))\n",
    "\n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "cosine_scores = [item[1] for item in results]\n",
    "z_scores = calculate_zscores(cosine_scores)\n",
    "#save the articles that were a good match\n",
    "articles = \"\"\n",
    "i=0\n",
    "first_z_score = z_scores[0]\n",
    "for item in results:\n",
    "    # Use the index from the Original dataframe to extract values of interest\n",
    "    abstract = chunk_df.iloc[item[0]]['raw_text']\n",
    "    # If the highest Z score is 2x this record then likey not as significant\n",
    "    if(first_z_score/2)<z_scores[i]:\n",
    "        #print(f\"Abtract: '{article_title}' with a cosine match of: {item[1]} and Z-score of: {z_scores[i]}\")\n",
    "        articles = articles + \" \" + abstract\n",
    "    i=i+1\n",
    "print(\"Here is the best answer I could find about \", query)\n",
    "print(best_answer(articles, query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba127b5-45f8-40cc-a980-7937a48c5729",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
