RAFT: Adapting Language Model to Domain Specific RAG“Open book”
query answer
“Closed book”
query answer
Bake in Knowledge
at Train Time
Model can use
External Docs at Test
RAFT (Proposed)
query answer
Teach Model to
use External Docs at Test
Figure 1: How best to prepare for an Exam?(a) Fine-tuning based approaches implement "studying" by either directly
"memorizing" the input documents or answering practice QA without referencing the documents. (b) Alternatively, in-
context retrieval methods fail to leverage the learning opportunity afforded by the fixed domain and are equivalent to
taking an open-book exam without studying. While these approaches leverage in-domain learning, they fail to prepare for
open-book tests. In contrast, our approach (c) RAFT leverages fine-tuning with question-answer pairs while referencing the
documents in a simulated imperfect retrieval setting — thereby effectively preparing for the open-book exam setting.
performance. RAFT aims to not only enable models to learn
domain specific knowledge through fine-tuning, but also
to ensure robustness against inaccurate retrievals. This is
achieved by training the models to understand the dynamics
between the question posed (prompt), the domain specific
documents retrieved, and the appropriate answer. Going
back to our analogy, our approach is analogous to study-
ing for an open-book exam by recognizing relevant, and
irrelevant retrieved documents.
In RAFT, we train the model to answer the question (Q)
from Document(s) (D*) to generate an answer (A*), where
A* includes chain-of-thought (Wei et al., 2022; Anthropic,
2023), and in the presence of distractor documents (Dk).
We explain the methodology in detail in Section 3 and ana-
lyze the sensitivity to the number of distractor documents
(k) at train- and test- time in Section 5. RAFT consis-
tently outperforms Supervised-finetuning both with- and
without- RAG across PubMed (Dernoncourt & Lee, 2017),
HotpotQA (Yang et al., 2018), and HuggingFace Hub, Torch
Hub, and Tensorflow Hub Gorilla datasets (Patil et al., 2023),
presenting a novel, yet simple technique to improve pre-
trained LLMs for in-domain RAG.
2. LLMs for Open-Book Exam
To understand our goal better, we expand on our analogy be-
tween training an LLM in the real-world setting of preparing
for an exam.
Closed-Book Exam A closed book exam often refers to
a scenario where the LLMs do not have access to any ad-
ditional documents or references to answer the questions
during the exam. For LLMs, this is equivalent to the sce-
nario, for example, in which the LLM is used as a chatbot.
In this scenario, the LLM draws from the knowledge baked
in during pre-training and supervised finetuning to respond
to the prompt.
Open Book Exam In contrast, we liken the open-book
exam setting to the scenario in which the LLM can re-
fer to external sources of information (e.g., a website or
a book chapter). In such scenarios, typically, the LLM is
paired with a retriever which retrieves ‘k’ documents (or
specific segments of the document) which are appended to
the prompt. It is only through these documents retrieved that
the LLM gains access to “new knowledge”. As a result, we
argue that the LLM’s performance in these settings, where it
is trained as a general-purpose LLM is largely dependent on
the quality of the retriever and how accurately the retriever
can identify the most relevant piece of information.
Domain Specific Open-Book Exam In this paper, we
focused on a narrower but increasingly popular domain than
the general open book exam, called the domain specific
open book exam. In domain specific open book exams, we
know apriori the domain in which the LLM will be tested
– used for inference. The LLM can respond to the prompt
using use any and all information from this specific domain,
which it has been fine-tuned on. Examples of domain spe-
cific examples include enterprise documents, latest news,
code repositories belonging to an organization, etc. In all
these scenarios, the LLM will be used to respond to the
questions, whose answers can be found within a collection
of documents (a small practical domain). The retrieval tech-
nique itself has little to no impact on the mechanism (though
it may impact the accuracy). This paper mainly studies this,
domain specific open-book setting and how to adapt a pre-
trained LLM to this specific domain, including how to make
it more robust to a varying number of retrieved documents
and distractors.
3. RAFT
In this section, we present RAFT, a novel way of training
LLMs for domain specific open-book exams. We first intro-
duce the classical technique of supervised fine-tuning, fol-
2
RAFT: Adapting Language Model to Domain Specific RAG
Figure 2: Overview of our RAFT method. The top-left figure depicts our approach of adapting LLMs to reading solution
from a set of positive and negative documents in contrast to standard RAG setup where models are trained based on the
retriever outputs, which is a mixture of both memorization and reading. At test time, all methods follow the standard RAG
setting, provided with a top-k retrieved documents in the context.
lowed by the key takeaways from our experiments. Then, we
introduce RAFT , a modified version of general instruction
tuning. Lastly, we provide an overview of the experiments
to expect in the later sections.
Supervised Finetuning
Consider the supervised fine-tuning (SFT) setting for a
Question-Answer dataset. The formulation consists of the
Dataset (D) from which a set of Question (Q) and corre-
sponding answer (A) pairs are derived or already available.
In the classical SFT setting, the model is trained to improve
its ability to answer the questions based on its knowledge -
obtained either during pre-training, or during the SFT train-
ing phase. The model so trained can also be used at test-time
with the Retrieval Augmented Generation (RAG) setting,
where additional documents can be introduced in the prompt
to help the model answer the question. This can be repre-
sented as follows:
• Train: Q → A
• 0-shot Inference: Q → A
• RAG Inference: Q + D → A
RAFT
Retrieval Aware Fine-Tuning (RAFT), presents a novel
recipe to prepare fine-tuning data to tailor the models for
domain specific open-book settings, equivalent to in-domain
RAG In RAFT, we prepare the training data such that each
data point contains a question (Q), a set of documents (Dk),
and a corresponding Chain-of-though style answer (A∗)
generated from one of the document (D∗). We differentiate
between two types of documents: ‘oracle’ documents (D∗)
i.e. the documents from which the answer to the question
can be deduced, and ‘distractor’ documents (Di) that do not
contain answer-relevant information. As an implementation
detail, the ‘oracle’ document doesn’t need to be a single doc-
ument, but can be more than one document, as is the case in
HotpotQA (Yang et al., 2018). Then, for P fraction of the
questions (qi) in the dataset, we retain the oracle document
(d∗
i ) along with distractor documents (dk−1). For (1 − P )
fraction of the questions (qi) in the dataset, we include no
oracle document and only include distractor documents (dk).
We then fine-tune the language model using the standard
supervised training (SFT) technique, training it to generate
answers from the provided documents and questions. Fig. 2
illustrates the high-level design principal for RAFT .
We demonstrate that our approach trains the model to per-
form better RAG on the set of documents it is trained on i.e.,
in-domain. By removing the oracle documents in some in-
stances, we are compelling the model to memorize answers
instead of deriving them from the context. The training data
for RAFT is as follows, and an example of training data can
be seen in Fig. 3:
• P % of data: Q + D∗ + D2 + . . . + Dk → A∗
• (1 − P) % of data: Q + D1 + D2 + . . . + Dk → A∗
Subsequently, for the test scenario, the model is provided
with the Q and top-k documents retrieved by the RAG
pipeline. Note that RAFT is independent of the retriever
used.
A key factor in enhancing training quality is the genera-
3
RAFT: Adapting Language Model to Domain Specific RAG
tion of a reasoning process, such as Chain-of-Thought, to
explain the provided answers.RAFT approach is similar:
we demonstrate that creating a full reasoning chain and in
addition, clearly citing sources enhances the model’s accu-
racy in answering questions. In Fig. 3, we illustrate this
set-up. Generating the training data in this fashion, involves
presenting the model with a question, context, and verified
answers, and then requesting it to form a reasoning chain
that appropriately references the original context.
For all the datasets in our experiments, we generate the
answers using the technique described above. Note that the
Gorilla APIBench dataset, already includes reasoning in
the answers. We provide an example of the generation step
in Fig. 3, the detailed reasoning answer includes a citation
from the original context inside ##begin_quote## and
##end_quote## as well as the detailed explanation on
how to reach the conclusion based on the citations. We
demonstrate that adding detailed reasoning paragraphs helps
boost the model’s performance in our experiment section.
4. Evaluation
We design our experiments to study how well RAFT per-
forms compared to various baselines. We find that the RAFT-
7B model (a finetuned version of LlaMA-2) is better at read-
ing and extracting information from in-domain documents,
than domain specific finetuned model, and general-purpose
model with RAG. As an ablation, we also demonstrate how
important it is for the model to learn with Chain-of-Thought
responses. In this section, we will first introduce all the
datasets we used in the experiments, then all the baseline
model/fine-tuning techniques that we benchmark against.
4.1. Datasets
In our experiments, we use the following datasets to evaluate
our model and all baselines. We selected these datasets
to represent both popular and diverse domains including
Wikipedia, Coding/API documents, and question-answering
on medical documents.
• Natural Questions (NQ) (Kwiatkowski et al., 2019),
Trivia QA (Joshi et al., 2017) and HotpotQA (Yang
et al., 2018) are the open-domain question-answers
based on Wikipedia, mainly focused on common
knowledge (e.g., movies, sports, etc).
• HuggingFace, Torch Hub, and TensorFlow Hub are
from the APIBench (Patil et al., 2023) proposed in
the Gorilla paper. These benchmarks measure how to
generate the correct, functional, and executable API
calls based on the documentation.
• PubMed QA (Jin et al., 2019) is a question-answering
dataset tailored only for biomedical-research question-
answering. It mainly focuses on answering medical
and biology questions based on a given set of docu-
ments.
Note that the first category of dataset (NQ, Trivia QA, and
HotpotQA) is a relatively general domain whereas the latter
two domains are on very domain specific documents.
Baselines We consider the following baselines for our
experiments:
• LlaMA2-7B-chat model with 0-shot prompting: this is
the commonly used instruction-finetuned model for QA
tasks, where we provide clearly written instructions,
but no reference documentation.
• LlaMA2-7B-chat model with RAG (Llama2 + RAG):
similar to the previous setting, except here we include
reference documents. This is a popular technique when
dealing with domain specific QA tasks.
• domain specific Finetuning with 0-shot prompting
(DSF): Performing standard supervised finetuning,
without documents in context. We find that it mostly
useful to align the answering style of the model as well
as get familiar with the domain context.
• domain specific Finetuning with RAG (DSF + RAG):
Equip a domain specific finetuned model with external
knowledge using RAG. So, for the “knowledge” the
model does not know, it can still refer to the context.
4.2. Results
Using the above datasets and baselines, we evaluate our
model RAFT and demonstrate the effectiveness of RAFT in
Tab. 1. We see that RAFT consistently and significantly
outperforms the baselines. Compared with the base Llama-
2 instruction-tuned model, RAFT with RAG does much
better in terms of extracting information as well as being
robust towards distractors. The gain can be as big as 35.25%
on Hotpot QA and 76.35% on Torch Hub evaluation. Com-
pared with DSF on the specific dataset, our model does bet-
ter at relying on the provided context to solve the problem.
RAFT does much better on tasks like HotpotQA and Hug-
gingFace datasets (30.87% on HotpotQA and 31.41% on
HuggingFace). Note that for PubMed QA, since it is a binary
yes/no question, we don’t observe significant gains when we
compare our model with DSF + RAG. Even compared with
a much larger and better model GPT-3.5, RAFT demon-
strates significant advantages.
Overall, the LLaMA-7B model, both with and without the
RAG, performs poorly due to its answering style not align-
ing with the ground truth. By applying domain specific
4
RAFT: Adapting Language Model to Domain Specific RAG
Question: The Oberoi family is part of a hotel company that has a head office in what
city?
context: [The Oberoi family is an Indian family that is famous for its involvement in
hotels, namely through The Oberoi Group]...[It is located in city center of Jakarta,
near Mega Kuningan, adjacent to the sister JW Marriott Hotel. It is operated by The
Ritz-Carlton Hotel Company. The complex has two towers that comprises a hotel and the
Airlangga Apartment respectively]...[The Oberoi Group is a hotel company with its head
office in Delhi.]
Instruction: Given the question, context and answer above, provide a logical reasoning
for that answer. Please use the format of: ##Reason: {reason} ##Answer: {answer}.
--------------------------------------------------------------------------------------
CoT Answer: ##Reason: The document ##begin_quote## The Oberoi family is an Indian
family that is famous for its involvement in hotels, namely through The Oberoi Group.
##end_quote## establishes that the Oberoi family is involved in the Oberoi group, and
the document ##begin_quote## The Oberoi Group is a hotel company with its head office
in Delhi. ##end_quote## establishes the head office of The Oberoi Group. Therefore,
the Oberoi family is part of a hotel company whose head office is in Delhi. ##Answer:
Delhi
Figure 3: RAFT prompt to help LLM evaluate its own generated reasoning and answers, contrasting them with the correct
reasoning and answers. The LLM is prompted to identify errors in its reasoning and extract key insights for improvement.
This figure specifically represents the ‘GenerateExplanation‘ step in the RAFT algorithm (Section 3).
Table 1: RAFT improves RAG performance forall specialized domains: Across PubMed, HotpotQA, HuggingFace,
Torch Hub, and Tensorflow Hub, we see that domain specific Finetuning improves significantly of the performance of the
base model, but RAFT consistently outperforms the existing domain specific finetuning method with or without RAG. This
suggests the need to train the model with context. We compare our model with LLaMA finetuning receipes, and provide
GPT-3.5 for reference.
PubMed HotpotQA HuggingFace Torch Hub TensorFlow Hub
GPT-3.5 + RAG 71.60 41.5 29.08 60.21 65.59
LLaMA2-7B 56.5 0.54 0.22 0 0
LLaMA2-7B + RAG 58.8 0.03 26.43 08.60 43.06
DSF 59.7 6.38 61.06 84.94 86.56
DSF + RAG 71.6 4.41 42.59 82.80 60.29
RAFT (LLaMA2-7B) 73.30 35.28 74.00 84.95 86.86
tuning, we significantly enhance its performance. This pro-
cess enables the model to learn and adopt the appropriate
style of answering. However, introducing RAG to a domain-
specifically fine-tuned (DSF) model doesn’t invariably lead
to better outcomes. This might indicate that the model lacks
training in context processing and extracting useful infor-
mation from it. By incorporating our method, RAFT , we
train the model not only to match its answering style with
that required but also to improve its document processing
capabilities. Consequently, our approach outperforms all
others.
4.3. Effect of CoT
We also conduct an analysis to evaluate the effectiveness of
the Chain-of-Thought approach in enhancing the model’s
performance. As indicated in Table 2, simply providing
the answer to a question may not always be adequate. This
approach can lead to a rapid decrease in loss, resulting in
the training process to diverge. Incorporating a reasoning
chain that not only guides the model to the answer but also
enriches the model’s understanding can improve the over-
all accuracy. In our experiments, integrating the Chain-of-
Thought significantly enhances training robustness. We em-
ploy GPT-4-1106 to generate our Chain-of-Thought prompts
5
RAFT: Adapting Language Model to Domain Specific RAG
and include an example of the prompt we used in Figure 3.
4.4. Qualitative Analysis
To illustrate the potential advantages of RAFT over the
domain-specifically fine-tuned (DSF) approach, we present
a comparative example in Figure 4. This example qual-
itatively demonstrates a scenario where the DSF model
becomes confused by a question asking for the identity of
a screenwriter. Instead of providing the correct name, it
mistakenly cites one of the films written by the screenwriter.
In contrast, the RAFT model accurately answers the ques-
tion. This discrepancy suggests that training a model solely
with question-answer pairs may impair its ability to derive
relevant context from provided documents. The comparison
underscores the importance of incorporating both standard
instructional tuning and context comprehension into the
training dataset to preserve and enhance the model’s ability
to process text effectively.
4.5. Should we train the LLM always with the oracle
context for RAG?
In our exploration of whether large language models
(LLMs) should always be trained with the oracle context for
Retrieval-Augmented Generation (RAG), we address a key
question: what proportion (p%) of the training data should
include oracle documents? Intuitively, one might assume
that for effective training in reading and extracting informa-
tion from context (e.g., RAG tasks), the oracle document
should always be included during training (P = 100%). How-
ever, our findings challenge this assumption: incorporating
a portion of the training data without the oracle document
in the context (P = 80%) appears to enhance the model’s
performance on RAG tasks.
Fig. 5 presents our investigation into the hyperparameter
P%, which represents the percentage of training instances
that should include oracle documents. Our analysis reveals
that the optimal proportion varies across datasets, with fig-
ures ranging from 40%, 60%, and 100%. This indicates
that training your LLM without the correct corresponding
context at times can be beneficial for the downstream task of
answering questions related to the documents. In our train-
ing setup, we include four distractor documents alongside
the oracle document, and at test time, we maintain this for-
mat by providing the oracle document with four distractors.
Our findings suggest that, for domain specific RAG tasks,
including a certain percentage of training data without the
oracle documents in the context proves to be advantageous.
5. RAFT Generalizes to Top-K RAG
After demonstrating the performance of RAFT on vari-
ous benchmarks, we now study another important problem:
How does the number of distractor documents in RAFT af-
fect the model’s performance when augmented with top-k
retriever augmented generation (RAG) result during the eval-
uation? Previous research has highlighted the vulnerability
of LLMs to irrelevant text (see studies (Shi et al., 2023a;
Weston & Sukhbaatar, 2023; Liu et al., 2023b)). This issue
is particularly critical for LLMs + RAG since top-k RAG
is frequently employed at test time to ensure high recall.
Such a scenario necessitates the model to have the ability to
discern and disregard irrelevant content, focusing solely on
pertinent information.
5.1. Making Model Robust to top-K RAG
To tackle the challenge of enhancing large language mod-
els’ (LLMs) ability to sift through irrelevant text within the
retrieval pipeline, our analysis revealed that training solely
with oracle (highly relevant) documents can inadvertently di-
minish the model’s ability to discern and disregard irrelevant
information. To address this, our algorithm, RAFT , adopts
a strategy that integrates oracle documents with a mix of
irrelevant ones. This methodology prompts us to investigate
the ideal fraction of negative (irrelevant) documents to in-
corporate throughout the training process and to assess how
well this training approach adapts to different volumes of
documents encountered by the Retrieval-Augmented Gen-
eration (RAG) during the test phase. Our aim is to refine
the balance between relevant and irrelevant information to
strengthen the model’s efficiency in identifying and utilizing
pertinent content. Notice that Sec 4.5 looked at what P% of
training data should include distractors, while in this section,
we study test-time scenarios.
Training with Negative Documents To enhance the robust-
ness of large language models (LLMs) against irrelevant
text in retrieved documents, we adopted a finetuning ap-
proach that incorporates both golden (highly relevant) docu-
ments and distractor (irrelevant) documents. The model was
trained with varying numbers of distractor documents, but
consistently evaluated using the top-k documents obtained
from the retriever - not to be confused with p.
Our findings, detailed in Fig. 6, reveal that finetuning with
only the oracle document frequently results in inferior per-
formance compared to configurations that include a greater
number of distractor documents. As we can see in the fig-
ure, the better performance for Natural Questions is training
with D∗ + 3D and it is D∗ + 1D documents with Hotpot
QA. This insight has been particularly beneficial for our
algorithm, RAFT . In our experiments, we typically employ
a training setup consisting of one oracle document alongside
four distractor documents. This approach strikes a balance,
ensuring the model is not overwhelmed by distractors while
still gaining the ability to effectively discern and prioritize
relevant information.
6
RAFT: Adapting Language Model to Domain Specific RAG
Table 2: Ablation on Chain-of-Thought: The numbers of RAFT and RAFT without CoT. Results on various datasets
show that adding CoT can significantly improve the performance of the finetuned model. With a gain of 9.66% and 14.93%
on the Hotpot QA and HuggingFace datasets respectively.
PubMed HotpotQA HuggingFace Torch Hub TensorFlow Hub
RAFT w.o CoT 68.30 25.62 59.07 86.56 83.21
RAFT 73.30 35.28 74.00 84.95 86.86
HotPot QA
Question: What screenwriter with credits for “Evolution”[0/1879]e a film starring Nicolas Cage and Téa Leoni?
Documents: . . . David Weissman is a screenwriter and director. His film credits include “The Family Man” (2000),
“Evolution” (2001), and “When in Rome” (2010).
. . .
The Family Man is a 2000 American romantic comedy-drama film directed by Brett Ratner, written by David Diamond
and David Weissman, and starring Nicolas Cage and Téa Leoni. . . .
——————————————————————————————————————————————–
RAFT: ##Reason: The screenwriter with credits for the film “Evolution,” starring Nicolas Cage and Téa Leoni, is
David Weissman. This information is provided in the reference documents which mention David Weissman as a
screenwriter with film credits including “The Family Man” (2000), “Evolution” (2001), and “When in Rome” (2010).
Therefore, the screenwriter for “Evolution” is David Weissman. ##Answer: David Weissman ✓
DSF: The Family Man ✗
Figure 4: Comparison of RAFT and DSF: We prompt RAFT and DSF fine-tuned models on the HotpotQA dataset. We
can see that the DSF model extracts the wrong information from the context. For the question, who is the screenwriter, it
responds with a film name. RAFT manages to get the result correctly .
Generalization to a variable number of test-time docu-
ments. We extended our research to examine the impact of
different quantities of test-time documents on the model’s
performance. Specifically, our experiments focused on as-
sessing how models, trained with varying numbers of dis-
tractor documents, respond to changes in the number of
documents presented at test time.
The results, illustrated in Fig. 6, confirm that the inclusion
of distractor documents during training indeed makes the
model more resilient to fluctuations in the number of docu-
ments encountered during testing. This ability to maintain
consistent performance despite variations in test-time doc-
ument numbers further validates the robustness of our ap-
proach, RAFT . This finding underscores the importance of
a well-calibrated training environment to prepare the model
for a range of scenarios it may encounter in real-world ap-
plications.
6. Related Works
Retrieval-Augmented Language Models RAG enhances
language models by integrating a retrieval module that
sources relevant information from external knowledge bases,
significantly improving performance across various NLP
tasks, including language modeling (Guu et al., 2020;
Borgeaud et al., 2022; Khandelwal et al., 2019; Shi et al.,
2023d; Lin et al., 2023b; Shi et al., 2023c; Asai et al., 2023;
Xu et al., 2023; Wang et al., 2023) and open-domain ques-
tion answering (Izacard et al., 2023; Lewis et al., 2020). This
integration follows a “retrieve-and-read" paradigm where
the retrieval module provides additional context from exter-
nal sources, which the LM then uses to generate the final out-
put. The retrieval process involves using the input as a query
to fetch documents, which the LM incorporates for final pre-
dictions. For instance, Atlas (Izacard et al., 2023) fine-tunes
T5 models with the retriever, treating documents as latent
variables, while RETRO (Borgeaud et al., 2022) modifies
the decoder-only architecture to include retrieved texts and
conducts pre-training from scratch. kNN-LM (Khandelwal
et al., 2019) interpolates between the LM’s next token distri-
bution and distributions computed from retrieved tokens at
inference. (Shi et al., 2023d; Ram et al., 2023) assume black-
box access to an LM and combine it with either off-the-shelf
or fine-tuned retriever.
Memorization A key question around large neural language
models is whether they truly “understand” text (Feldman,
7
RAFT: Adapting Language Model to Domain Specific RAG0 20 40 60 80 100
P % Golden Retrieved Context at Training
0.25
0.30
0.35
0.40
0.45
Final Accuracy
Test Domain: NQ0 20 40 60 80 100
% Golden Retrieved Context at Training
0.50
0.55
0.60
0.65
Final Accuracy
Test Domain: TQA0 20 40 60 80 100
P % Golden Retrieved Context at Training
0.40
0.45
0.50
0.55
0.60
Final Accuracy
Test Domain: Hotpot QA
Figure 5: How many golden documents to involve? We study the hyperparameter P % which indicates what fraction of
the training data contains the oracle document(s) in its context. Results on NQ, TQA and HotpotQA suggest that mixing a
fraction of data that does not have the oracle document in its context is helpful for in-domain RAG.2 4 6 8 10
# Test Documents (Top-k)
0.22
0.24
0.26
0.28
0.30
0.32
Final Accuracy
Natural Questions
Train D*
Train D* + 1D
Train D* + 2D
Train D* + 3D2 4 6 8 10
# Test Documents (Top-k)
0.125
0.150
0.175
0.200
0.225
0.250
Final Accuracy
Hotpot QA
Train D*
Train D* + 1D
Train D* + 2D
Train D* + 3D
Figure 6: Test-Time Documents Varying: We study how robust RAFT is to varying numbers of test-time documents
that a retriever might provide. In NQ, we find that training with 4 documents leads to the best performance, but training
with 2 documents is optimal for HotpotQA. However, across both datasets, training with all datasets consisting of oracle
documents hurts performance.
2020; Power et al., 2022) or simply rely on surface pattern
memorization (Carlini et al., 2019; Tänzer et al., 2022).
(Feldman, 2020; Carlini et al., 2019; 2022) develop method-
ologies to quantify the extent of memorization in neural
models. (Brown et al., 2020; Power et al., 2022; Liu et al.,
2022b) further explored how memorization impacts the mod-
els’ generalization capabilities. Recently, a seminal work
by (Carlini et al., 2021; Shi et al., 2023b) demonstrated
the ability of language models to memorize and regurgitate
training data, raising significant privacy concerns (Kandpal
et al., 2022; Pan et al., 2020).
Finetuning of LLMs Recent years have seen rapid progress
in developing large-scale language models (LLMs) (Brown
et al., 2020; OpenAI, 2023; Workshop et al., 2022; Tou-
vron et al., 2023;?; Anil et al., 2023). To adapt these foun-
dation models to downstream tasks, fine-tuning (Mishra
et al., 2021; Sanh et al., 2021; Chung et al., 2022; Muen-
nighoff et al., 2023; Zhou et al., 2023b; Lin et al., 2023b;
Ji et al., 2024) has become a prevalent approach. Tradi-
tional supervised fine-tuning may be limited by the cost
and compute required for adapating LLMs. Addressing
these challenges, research in the realm of parameter-efficient
fine-tuning (Houlsby et al., 2019), such as Prompt Tuning
(Lester et al., 2021), Prefix-Tuning (Li & Liang, 2021),
P-Tuning (Liu et al., 2022a) and Low-Rank based fine-
tuning (Hu et al., 2021), has gained traction. These methods
enable LLMs to acquire domain-specific knowledge and
adapt to specialized tasks such as question answering, sum-
marization, and dialogue generation. Another branch of
finetuning is through RLHF (Ouyang et al., 2022; Rafailov
et al., 2023; Liu et al., 2023a; Zhang et al., 2023), which
adopts RL to align LLM’s preference with human.
Finetuning for RAG More recently, several papers have
been exploring the idea of finetuning a pretrained LLM to
be better at RAG tasks (Lin et al., 2023a; Wang et al., 2023;
Xu et al., 2023; Liu et al., 2024). These works focus on con-
structing a combination of finetuning dataset for RAG and
train a model to perform well on these tasks. In particular, in
8
RAFT: Adapting Language Model to Domain Specific RAG
their settings, at test time, the domain or documents can be
different than the training time; whereas our paper studies a
slightly opposite scenario where we only care about testing
the LLM on the same set of documents.
7. Conclusion
RAFT is a training strategy designed to enhance the model’s
performance in answering questions within a specific do-
main, in "open-book" settings. This technique demonstrates
a fine-tuning recipe for LLMs for question-answering tasks
based on a selected collection of documents. We have pin-
pointed several crucial design decisions, such as training
the model alongside distractor documents, organizing the
dataset so a portion lacks oracle documents in their con-
text, and formulating answers in a chain-of-thought manner
with direct quotations from the relevant text. Our evalua-
tions on PubMed, HotpotQA, and Gorilla API Bench un-
derline RAFT’s significant potential. Looking forward, we
anticipate that in-domain Retrieval-Augmented Generation
(RAG) will continue to gain interest within both industrial
and academic spheres. Unlike general-RAG, our work ad-
dresses practical scenarios where LLMs are tasked with an-
swering questions using domain-specific knowledge. Align-
ing with current trends, our findings suggest that smaller,
fine-tuned models are capable of performing comparably
well in domain-specific question-answering tasks, in con-
trast to their generic LLM counterparts.
References
Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin,
D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen,
Z., et al. Palm 2 technical report. arXiv preprint
arXiv:2305.10403, 2023.
Anthropic. Prompt engineering for claude’s long context
window. 2023.
Asai, A., Wu, Z., Wang, Y., Sil, A., and Hajishirzi, H. Self-
rag: Learning to retrieve, generate, and critique through
self-reflection. arXiv preprint arXiv:2310.11511, 2023.
Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford,
E., Millican, K., Van Den Driessche, G. B., Lespiau, J.-B.,
Damoc, B., Clark, A., et al. Improving language models
by retrieving from trillions of tokens. In International
conference on machine learning, pp. 2206–2240. PMLR,
2022.
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. Language models are few-shot learners.
Advances in neural information processing systems, 33:
1877–1901, 2020.
Carlini, N., Liu, C., Erlingsson, Ú., Kos, J., and Song,
D. The secret sharer: Evaluating and testing unintended
memorization in neural networks. In 28th USENIX Se-
curity Symposium (USENIX Security 19), pp. 267–284,
2019.
Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-
Voss, A., Lee, K., Roberts, A., Brown, T., Song, D.,
Erlingsson, U., et al. Extracting training data from large
language models. In 30th USENIX Security Symposium
(USENIX Security 21), pp. 2633–2650, 2021.
Carlini, N., Ippolito, D., Jagielski, M., Lee, K., Tramer, F.,
and Zhang, C. Quantifying memorization across neural
language models. In The Eleventh International Confer-
ence on Learning Representations, 2022.
Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,
Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma,
S., et al. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416, 2022.
Dernoncourt, F. and Lee, J. Y. Pubmed 200k rct: a dataset
for sequential sentence classification in medical abstracts.
arXiv preprint arXiv:1710.06071, 2017.
Feldman, V. Does learning require memorization? a short
tale about a long tail. In Proceedings of the 52nd Annual
ACM SIGACT Symposium on Theory of Computing, pp.
954–959, 2020.
Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M.
Retrieval augmented language model pre-training. In
International conference on machine learning, pp. 3929–
3938. PMLR, 2020.
Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B.,
De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and
Gelly, S. Parameter-efficient transfer learning for nlp.
In International Conference on Machine Learning, pp.
2790–2799. PMLR, 2019.
Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,
S., Wang, L., and Chen, W. Lora: Low-rank adaptation of
large language models. arXiv preprint arXiv:2106.09685,
2021.
Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni,
F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and
Grave, E. Atlas: Few-shot learning with retrieval aug-
mented language models. Journal of Machine Learning
Research, 24(251):1–43, 2023. URL http://jmlr.
org/papers/v24/23-0037.html.
Ji, C. C.-J., Mao, H., Yan, F., Shishir G. Patil, T. Z., Stoica,
I., and Gonzalez, J. E. Gorilla openfunctions v2. 2024.
9
RAFT: Adapting Language Model to Domain Specific RAG