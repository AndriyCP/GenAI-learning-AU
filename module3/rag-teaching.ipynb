{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edf86a47-b70a-498a-8dbc-de917d47fa84",
   "metadata": {},
   "source": [
    "### Retrival Augmentented Generation\n",
    "Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts.\n",
    "\n",
    "#### Vector embeddings\n",
    "Let's begin with the fundamental building blocks of how retrieval works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7347c523-adac-43c8-b651-acc6281afd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain_cohere -q\n",
    "%pip install spacy -q\n",
    "#ignore error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13d4086-c974-496a-99e1-2d5584e0755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now you need to run this in a terminal window\n",
    "# python -m spacy download en_core_web_md\n",
    "# now restart your kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4a4fed-79a7-4616-96b5-813c6f5bcc9a",
   "metadata": {},
   "source": [
    "Standard imports for the libraires we will be using in this notebook.  Try to keep your imports in the first cell so this can this code can more easliy be converted into a python program later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1513731f-ab9f-4e74-9249-e8ff08dfd433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import pyarrow\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain_community.embeddings.spacy_embeddings import SpacyEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "\n",
    "# Create the AWS client for the Bedrock runtime with boto3\n",
    "aws_client = boto3.client(service_name=\"bedrock-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceec844-3eee-4ce4-993d-d72ab9dd8560",
   "metadata": {},
   "source": [
    "#### Lets define functions that will use various embedding models so we can generate vector embeddings\n",
    "Spacey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1b6024-dda4-4dd3-836d-5830b472d8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spacy_vector_embedding(text):\n",
    "    embedder = SpacyEmbeddings(model_name=\"en_core_web_md\")\n",
    "    query_embedding = embedder.embed_query(text)\n",
    "\n",
    "    return(np.array(query_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf40345-ad82-4d53-bfa6-19360a03c4de",
   "metadata": {},
   "source": [
    "Huggingface Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb9c64d-f1a7-4123-b197-05d485553761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_huggingface_vector_embedding(text):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"bert-base-uncased\")\n",
    "    embedded_texts = embeddings.embed_query(text_data)\n",
    "    return(embedded_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35157a12-af69-4d79-8a9f-d7229eef4bec",
   "metadata": {},
   "source": [
    "Cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14383a5-eec2-40e7-9c61-ccadb437d46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_string_size(x, max_chars=2048):\n",
    "    # Check if the input is a string\n",
    "    if isinstance(x, str):\n",
    "        return x[:max_chars]\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0667bf3-01ef-4493-8cbc-744413c0d457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send in an array size of one and only return the 0th element\n",
    "def generate_cohere_vector_embedding(text_data):\n",
    "    input_type = \"clustering\"\n",
    "    truncate = \"NONE\" # optional\n",
    "    model_id = \"cohere.embed-english-v3\" # or \"cohere.embed-multilingual-v3\"\n",
    "    trunc_data = limit_string_size(text_data)\n",
    "    # Create the JSON payload for the request\n",
    "    json_params = {\n",
    "            'texts': [trunc_data],\n",
    "            'truncate': truncate, \n",
    "            \"input_type\": input_type\n",
    "        }\n",
    "    json_body = json.dumps(json_params)\n",
    "    params = {'body': json_body, 'modelId': model_id,}\n",
    "    \n",
    "    # Invoke the model and print the response\n",
    "    result = aws_client.invoke_model(**params)\n",
    "    response = json.loads(result['body'].read().decode())\n",
    "    return(np.array(response['embeddings'][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef0f33d-429d-4636-9082-d7bbb9850790",
   "metadata": {},
   "source": [
    "Amazon Titan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5794765f-0b32-451e-a145-6a7b4ce785e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate a dense vector using Amazon Titan with LangChain\n",
    "def generate_titan_vector_embedding(text):\n",
    "    #create an Amazon Titan Text Embeddings client\n",
    "    embeddings_client = BedrockEmbeddings(region_name=\"us-west-2\") \n",
    "\n",
    "    #Invoke the model\n",
    "    embedding = embeddings_client.embed_query(text)\n",
    "    return(np.array(embedding))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff40e7e-c880-4dd1-b7ad-70f8a12efe2a",
   "metadata": {},
   "source": [
    "This is the mathmatical formula to calcuate cosine similarity between 2 vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8367a7-c786-41fc-ad05-d53b5633d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "    return similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4047ec8c-4391-4504-b444-783e00f64f08",
   "metadata": {},
   "source": [
    "Now let's generate vectors for simple words and compare how we can compute cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75541e21-0738-46c6-87f2-ca48e26d7862",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Titan\n",
    "king_vector = generate_titan_vector_embedding(\"King\")\n",
    "queen_vector = generate_titan_vector_embedding(\"Queen\")\n",
    "man_vector = generate_titan_vector_embedding(\"man\")\n",
    "woman_vector = generate_titan_vector_embedding(\"woman\")\n",
    "print(f\"This embedding has {len(king_vector)} dimensions\")\n",
    "print(king_vector[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1403336c-54d9-47ca-b6aa-71e33d3ff78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_queen_vector = king_vector - man_vector + woman_vector\n",
    "\n",
    "similarity = cosine_similarity(calculated_queen_vector, queen_vector)\n",
    "print(f\"Cosine Similarity distance between Titan King - Queen: {similarity:.4f}\")\n",
    "\n",
    "similarity = cosine_similarity(king_vector, queen_vector)\n",
    "print(f\"Cosine Similarity of Titan King to Queen: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ce1c26-96bd-4ad0-9363-43652357245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input cohere for embedding \n",
    "king_vector = generate_cohere_vector_embedding('King')\n",
    "queen_vector = generate_cohere_vector_embedding(\"Queen\")\n",
    "man_vector = generate_cohere_vector_embedding(\"man\")\n",
    "woman_vector = generate_cohere_vector_embedding(\"woman\")\n",
    "print(f\"This embedding has {len(king_vector)} dimensions\")\n",
    "print(king_vector[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32463138-8d0c-4029-a3db-47df1b4aedc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_queen_vector = king_vector - man_vector + woman_vector\n",
    "\n",
    "similarity = cosine_similarity(calculated_queen_vector, queen_vector)\n",
    "print(f\"Cosine Similarity distance between Cohere King - Queen: {similarity:.4f}\")\n",
    "\n",
    "similarity = cosine_similarity(king_vector, queen_vector)\n",
    "print(f\"Cosine Similarity of Cohere King to Queen: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa44abe0-4283-4ec9-8a00-182dad13a06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spacy\n",
    "king_vector = generate_spacy_vector_embedding(\"King\")\n",
    "queen_vector = generate_spacy_vector_embedding(\"Queen\")\n",
    "man_vector = generate_spacy_vector_embedding(\"man\")\n",
    "woman_vector = generate_spacy_vector_embedding(\"woman\")\n",
    "print(f\"This embedding has {len(king_vector)} dimensions\")\n",
    "print(king_vector[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a9fc52-a659-4270-bab1-370fb67e64a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_queen_vector = king_vector - man_vector + woman_vector\n",
    "\n",
    "similarity = cosine_similarity(calculated_queen_vector, queen_vector)\n",
    "print(f\"Cosine Similarity distance between Spacey King - Queen: {similarity:.4f}\")\n",
    "\n",
    "similarity = cosine_similarity(king_vector, queen_vector)\n",
    "print(f\"Cosine Similarity of Spacey King to Queen: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1110c8bd-2a42-4541-bb17-36dcfd7b2890",
   "metadata": {},
   "source": [
    "Let's examine other phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21063601-c5c2-449d-b6ca-6b1b6aee6c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = cosine_similarity(generate_titan_vector_embedding(\"cat\"), generate_titan_vector_embedding(\"book\"))\n",
    "print(f\"Cosine Similarity of cat to book using Titan: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7a6da4-8667-4eaf-beeb-764883ac0031",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = cosine_similarity(generate_cohere_vector_embedding(\"cat\"), generate_cohere_vector_embedding(\"book\"))\n",
    "print(f\"Cosine Similarity of cat to book using Cohere: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbe9ee1-905c-425b-b44e-81fc1fa5ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = cosine_similarity(generate_spacy_vector_embedding(\"cat\"), generate_spacy_vector_embedding(\"book\"))\n",
    "print(f\"Cosine Similarity of cat to book using Spacey: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420fad29-9e38-4434-a200-e8fd414003c9",
   "metadata": {},
   "source": [
    "Now let's look at a larger sentences and see how larger models with more complexity handle the same task\n",
    "Here are 2 sentences that semantically similar but use different words and phrasing.\n",
    "\n",
    "1. The majestic, towering skyscrapers, their gleaming windows reflecting the golden rays of the setting sun, stood as a testament to human ingenuity and the \n",
    " indomitable spirit of progress, while the bustling streets below teemed with life as people from all walks of life hurried to their destinations, their faces \n",
    " a mix of determination and weariness, yet each individual contributing to the vibrant tapestry of the city's existence.\n",
    "\n",
    "2. The awe-inspiring, colossal high-rises, their polished glass facades mirroring the warm, amber glow of the fading daylight, served as a powerful symbol of human\n",
    " innovation and the unyielding drive for advancement, as the lively thoroughfares beneath pulsed with energy, filled with individuals from diverse backgrounds rushing\n",
    " to their intended locations, their expressions an amalgamation of resolve and fatigue, yet all playing a vital role in the dynamic, intricate mosaic that shaped the \n",
    " city's vibrant identity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f5da62-2cbd-4d40-b02b-57b89bba9cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"The majestic, towering skyscrapers, their gleaming windows reflecting the golden rays of the setting sun, stood as a testament to human ingenuity and the indomitable spirit of progress, while the bustling streets below teemed with life as people from all walks of life hurried to their destinations, their faces a mix of determination and weariness, yet each individual contributing to the vibrant tapestry of the city's existence.\"\n",
    "sentence2 = \"The awe-inspiring, colossal high-rises, their polished glass facades mirroring the warm, amber glow of the fading daylight, served as a powerful symbol of human innovation and the unyielding drive for advancement, as the lively thoroughfares beneath pulsed with energy, filled with individuals from diverse backgrounds rushing to their intended locations, their expressions an amalgamation of resolve and fatigue, yet all playing a vital role in the dynamic, intricate mosaic that shaped the city's vibrant identity.\"\n",
    "similarity = cosine_similarity(generate_spacy_vector_embedding(sentence1), generate_spacy_vector_embedding(sentence2))\n",
    "print(f\"Cosine Similarity of S1 to S2 using Spacey: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fdd6ba-7828-482b-9636-908ebeba22cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"The majestic, towering skyscrapers, their gleaming windows reflecting the golden rays of the setting sun, stood as a testament to human ingenuity and the indomitable spirit of progress, while the bustling streets below teemed with life as people from all walks of life hurried to their destinations, their faces a mix of determination and weariness, yet each individual contributing to the vibrant tapestry of the city's existence.\"\n",
    "sentence2 = \"The awe-inspiring, colossal high-rises, their polished glass facades mirroring the warm, amber glow of the fading daylight, served as a powerful symbol of human innovation and the unyielding drive for advancement, as the lively thoroughfares beneath pulsed with energy, filled with individuals from diverse backgrounds rushing to their intended locations, their expressions an amalgamation of resolve and fatigue, yet all playing a vital role in the dynamic, intricate mosaic that shaped the city's vibrant identity.\"\n",
    "similarity = cosine_similarity(generate_titan_vector_embedding(sentence1), generate_titan_vector_embedding(sentence2))\n",
    "print(f\"Cosine Similarity of S1 to S2 using Titan: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bce6b2e-5ee0-405e-aef2-d8635133663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"The majestic, towering skyscrapers, their gleaming windows reflecting the golden rays of the setting sun, stood as a testament to human ingenuity and the indomitable spirit of progress, while the bustling streets below teemed with life as people from all walks of life hurried to their destinations, their faces a mix of determination and weariness, yet each individual contributing to the vibrant tapestry of the city's existence.\"\n",
    "sentence2 = \"The awe-inspiring, colossal high-rises, their polished glass facades mirroring the warm, amber glow of the fading daylight, served as a powerful symbol of human innovation and the unyielding drive for advancement, as the lively thoroughfares beneath pulsed with energy, filled with individuals from diverse backgrounds rushing to their intended locations, their expressions an amalgamation of resolve and fatigue, yet all playing a vital role in the dynamic, intricate mosaic that shaped the city's vibrant identity.\"\n",
    "similarity = cosine_similarity(generate_cohere_vector_embedding(sentence1), generate_cohere_vector_embedding(sentence2))\n",
    "print(f\"Cosine Similarity of S1 to S2 using Cohere: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da3a31a-406d-417a-b752-f3c3afd0a66f",
   "metadata": {},
   "source": [
    "#### Right model for the right job\n",
    "Bigger models aren't necessarily more capable for all tasks.  On simple sentences Spacey results show it to be the best model for this task.  Let's switch to a more complex task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7230a6-86c4-4950-a793-6bae2bb4925d",
   "metadata": {},
   "source": [
    "#### Persistance of embeddings for later retrieval\n",
    " In order to do semantic search and retrieve relevant content we need to store that content for later use.  We can store the embedding in several different persistence technologies.  To start simply let's store the data in memory using pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6763ad27-f4b8-4f6a-8200-d067332ea4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_value(value):\n",
    "    value_str = str(value)\n",
    "    cleaned_value = ''.join(char for char in value_str if char.isalnum() or char.isspace())\n",
    "    return cleaned_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6b1080-7652-4333-ad30-ad57d36d5868",
   "metadata": {},
   "source": [
    "Using Gaggle I looked for an interesting dataset that had data that I wanted to use to answer specific questions that is likely not in the large corpus of trained data. Let's put the data into a pandas dataframe and examine the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d11db83-cffa-4ebc-8f41-5392bb92cc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean abstract text\n",
    "df = pd.read_csv('data/latest_research_articles.csv')\n",
    "df['abstract'] = df['abstract'].apply(clean_value)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dfb897-a1aa-4a82-8ac0-3825eefb1afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets save the dataframe in 4 separate so we can parallalize the embedding\n",
    "# df1, df2, df3, df4 = np.array_split(df, 4)\n",
    "# df1.to_pickle('data/non-embedded_df1.pkl')\n",
    "# df2.to_pickle('data/non-embedded_df2.pkl')\n",
    "# df3.to_pickle('data/non-embedded_df3.pkl')\n",
    "# df4.to_pickle('data/non-embedded_df4.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540a5b32-5c78-48ea-b4cc-26ed804bc043",
   "metadata": {},
   "source": [
    "Now let's create a new column that represents the article abstract as a vector embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8695452-a792-4a38-81b3-dd6d554d60ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings using spaCy\n",
    "# Add a new column 'embedded_abstract' by applying the function to an existing column\n",
    "# This step takes a while so I did it for you and saved the output as pickle\n",
    "#df['embedded_abstract'] = df['abstract'].apply(generate_spacy_vector_embedding)\n",
    "#df.to_pickle('data/spacey_embedded.pkl')\n",
    "\n",
    "# Easier way\n",
    "dfs = pd.read_pickle('data/spacey_embedded.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1516c06b-233a-491d-81cb-063013875f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also generated Spacey embedding so we can compare\n",
    "# df1 = pd.read_pickle('data/embedded_spacey_df1.pkl')\n",
    "# df2 = pd.read_pickle('data/embedded_spacey_df2.pkl')\n",
    "# df3 = pd.read_pickle('data/embedded_spacey_df3.pkl')\n",
    "# df4 = pd.read_pickle('data/embedded_spacey_df4.pkl')\n",
    "# Combine the dataframes that have been embedded then combine\n",
    "#dfs = pd.concat([df1, df2, df3, df4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08975b47-f547-4930-a7e4-42876c00e7e9",
   "metadata": {},
   "source": [
    "### Retrieval from embedded sources\n",
    "Now that we have a dataframe with embedded content of interest, we can use semantic similarity to retrieve the right data to feed to an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b25889-1d6a-4c2f-bd19-5d117cff23f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's setup a query that a used might ask\n",
    "query = \"What is the latest research for broken ribs in children\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f13dbb-b9b9-4063-9d5a-1c229a83d0e3",
   "metadata": {},
   "source": [
    "#### Spacey Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d92007-07f0-45d0-a267-93f83f23beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's search our records for a good semantic search\n",
    "# To compare the vectors we need to embed the string we want to compare and convert it to a vector\n",
    "query_vector = generate_spacy_vector_embedding(query)\n",
    "\n",
    "# Search through each each row and calculate cosine similarity and store the results in an array of tuples\n",
    "results = []\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in dfs.iterrows():\n",
    "    # Extract the value from the specified column\n",
    "    article_embedding = row['embedded_abstract']\n",
    "    results.append((index, cosine_similarity(article_embedding, query_vector)))\n",
    "\n",
    "# Sort the stored results from highest to lowest\n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "i = 0\n",
    "# Print the first 5 sorted items with article index and title\n",
    "print(\"Here are a few articles that may match your interest:\")\n",
    "for item in results:\n",
    "    article_title = dfs.iloc[item[0]]['title']\n",
    "    print(f\"Abtract: '{article_title}' with a cosine match of: {item[1]}\")\n",
    "    i=i+1\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b0949d-dad2-4fa3-b6cc-a11c5a61fc09",
   "metadata": {},
   "source": [
    "#### Titan Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43021e17-eede-416e-8b84-aa65b30e8fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings using spaCy\n",
    "# Add a new column 'embedded_abstract' by applying the function to an existing column\n",
    "# This step takes a while so I did it for you and saved the output as pickle\n",
    "#dft = df.copy()\n",
    "#dft['embedded_abstract'] = dft['abstract'].apply(generate_titan_vector_embedding)\n",
    "#df.to_pickle('data/spacey_embedded.pkl')\n",
    "\n",
    "# Easier way\n",
    "dft = pd.read_pickle('data/embedded_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02d48ef-2c13-4f4f-9389-76b60de4ffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's search our records for a good semantic search\n",
    "query_vector = generate_titan_vector_embedding(query)\n",
    "\n",
    "results = []\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in dft.iterrows():\n",
    "    # Extract the value from the specified column\n",
    "    article_embedding = row['embedded_abstract']\n",
    "    results.append((index, cosine_similarity(article_embedding, query_vector)))\n",
    "    #print (index, value)\n",
    "\n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "i = 0\n",
    "# Print the sorted data\n",
    "print(\"Here are a few articles that may match your interest:\")\n",
    "for item in results:\n",
    "    article_title = dft.iloc[item[0]]['title']\n",
    "    print(f\"Abtract: '{article_title}' with a cosine match of: {item[1]}\")\n",
    "    i=i+1\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858166f7-846f-411b-8cd6-7f147ae62128",
   "metadata": {},
   "source": [
    "#### Cohere Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f3af81-5d8c-4fe6-95d1-6ac172969a44",
   "metadata": {},
   "source": [
    "Each of these models also have a [max input tokens limit](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-embed.html) and that determines the largest number of characters of text we can embed.\n",
    "\n",
    "[https://cohere.com/blog/introducing-embed-v3](https://cohere.com/blog/introducing-embed-v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2815ed-ffe2-469a-96aa-b73e9d642a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings using Cohere\n",
    "# Add a new column 'embedded_abstract' by applying the function to an existing column\n",
    "# This step takes a while so I did it for you and saved the output as pickle\n",
    "#df['embedded_abstract'] = df['abstract'].apply(generate_cohere_vector_embedding)\n",
    "#df.to_pickle('data/cohere_embedded.pkl')\n",
    "\n",
    "# This step takes a while so I did it for you and saved the output as pickle\n",
    "dfc = pd.read_pickle('data/cohere_embedded.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ca5cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's search our records for a good semantic search\n",
    "query_vector = generate_cohere_vector_embedding(query)\n",
    "\n",
    "results = []\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in dfc.iterrows():\n",
    "    # Extract the value from the specified column\n",
    "    article_embedding = row['embedded_abstract']\n",
    "    results.append((index, cosine_similarity(article_embedding, query_vector)))\n",
    "    #print (index, value)\n",
    "\n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "i = 0\n",
    "# Print the sorted data\n",
    "print(\"Here are a few articles that may match your interest:\")\n",
    "for item in results:\n",
    "    article_title = dfc.iloc[item[0]]['title']\n",
    "    print(f\"Abtract: '{article_title}' with a cosine match of: {item[1]}\")\n",
    "    i=i+1\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b6c4e8-e7f1-40a0-b34a-84e5d67a86e5",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    " For this task Titan performed much better in this scenario.  There are many models out there, before you begin to solve a problem with these new models that are changing daily take some time to test out a few before you land on a solution set.\n",
    " \n",
    " Here is a list of benchmarks for large embedded text models, most models nw have benchmarks with model performance.\n",
    "\n",
    " [Massive Text Embedding Benchmark](https://huggingface.co/spaces/mteb/leaderboard)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
